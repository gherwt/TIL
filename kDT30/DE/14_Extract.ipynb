{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50d6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hdfs\n",
    "# ref : https://hdfscli.readthedocs.io/en/latest/quickstart.html#python-bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d45deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient # 하둡 서버와 연결하는 client 객체 모듈(hdfs 접근)\n",
    "import requests # web api 사용 위함\n",
    "import json # json 형식으로 바꿔서 저장하기 위해 사용\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f5584",
   "metadata": {},
   "source": [
    "### DataW에 DataL 의 필요 data 를 이동 저장 단계\n",
    "- Lake 에서 HDFS로 이동 저장\n",
    "1. 코로나 발생현황(일간데이터) : api 수집\n",
    "2. 코로나 예방접종현황(일간데이터) : web 페이지 크롤링\n",
    "3. 인구 사회 데이터 : 파일 데이터\n",
    "- 1, 2번 데이터는 json 형식으로 변환 후 hdfs에 저장\n",
    "- 3번 데이터는 파일 그대로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f342ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hdfs client 연결객체 생성\n",
    "client = InsecureClient('http://localhost:9870', user = 'root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712a763",
   "metadata": {},
   "source": [
    "## hdfs로 부터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92556591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client 객체 사용 file 읽어오기\n",
    "with client.read('/rdd/score.txt') as reader :\n",
    "    score = reader.read() # byte code 로 읽어옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e12751ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xea\\xb9\\x80\\xec\\xb2\\xa0\\xec\\x88\\x98 \\xec\\x8a\\xa4\\xed\\x8c\\x8c\\xed\\x81\\xac 50\\r\\n\\xed\\x99\\x8d\\xea\\xb8\\xb8\\xeb\\x8f\\x99 \\xec\\x8a\\xa4\\xed\\x8c\\x8c\\xed\\x81\\xac 80\\r\\n\\xec\\x9e\\x84\\xea\\xba\\xbd\\xec\\xa0\\x95 \\xec\\x8a\\xa4\\xed\\x8c\\x8c\\xed\\x81\\xac 60\\r\\n\\xec\\x9e\\x84\\xec\\x9a\\x94\\xed\\x99\\x98 \\xed\\x85\\x90\\xec\\x84\\x9c\\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x9a\\xb0 100\\r\\n\\xed\\x99\\x8d\\xec\\xa7\\x84\\xed\\x98\\xb8 \\xed\\x85\\x90\\xec\\x84\\x9c\\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x9a\\xb0 22\\r\\n\\xed\\x99\\x8d\\xec\\xa7\\x84\\xed\\x98\\xb8 \\xed\\x85\\x90\\xec\\x84\\x9c\\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x9a\\xb0 22\\r\\n\\xec\\x9d\\xb4\\xec\\x9c\\xa4\\xec\\x97\\xb4 \\xed\\x85\\x90\\xec\\x84\\x9c\\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x9a\\xb0 90\\r\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890bb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수 스파크 50\r\n",
      "홍길동 스파크 80\r\n",
      "임꺽정 스파크 60\r\n",
      "임요환 텐서플로우 100\r\n",
      "홍진호 텐서플로우 22\r\n",
      "홍진호 텐서플로우 22\r\n",
      "이윤열 텐서플로우 90\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/ko/3/library/stdtypes.html#bytes-and-bytearray-operations\n",
    "score_data = bytes.decode(score)\n",
    "print(score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef087e",
   "metadata": {},
   "source": [
    "## hdfs에 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "458c67df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:hdfs.util:Exception in child.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/util.py\", line 76, in consumer\n",
      "    self._consumer(data)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\", line 515, in consumer\n",
      "    raise _to_error(res)\n",
      "hdfs.util.HdfsError: Permission denied: user=root, access=WRITE, inode=\"/user\":lab09:supergroup:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n",
      "\n"
     ]
    },
    {
     "ename": "HdfsError",
     "evalue": "Permission denied: user=root, access=WRITE, inode=\"/user\":lab09:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b7316da31e81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/corona_data/sido_area.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CP949'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/corona_data/sido_area_tmp.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CP949'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/util.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_err\u001b[0m \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Child terminated without errors.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/util.py\u001b[0m in \u001b[0;36mconsumer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting consumer.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exception in child.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mconsumer\u001b[0;34m(_data)\u001b[0m\n\u001b[1;32m    513\u001b[0m       )\n\u001b[1;32m    514\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_to_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHdfsError\u001b[0m: Permission denied: user=root, access=WRITE, inode=\"/user\":lab09:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n"
     ]
    }
   ],
   "source": [
    "# hdd(note 디렉터리)에 있는 data 를 hdfs 로 이동\n",
    "# 1. 로컬 데이터 파일을 열기\n",
    "# 2. hdfs client 통해서 쓰기 모듈 사용\n",
    "# 3. 로컬 파일의 내용을 한줄씩 읽어서 -> hdfs에 쓰기\n",
    "with open('./data/corona_data/sido_area.csv',encoding='CP949') as reader, client.write('./data/corona_data/sido_area_tmp.csv') as writer:\n",
    "    for line in reader :\n",
    "        writer.write(line.encode('CP949'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a45228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:hdfs.client:Error while uploading. Attempting cleanup.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\", line 643, in upload\n",
      "    _upload(path_tuple)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\", line 574, in _upload\n",
      "    self.write(_temp_path, wrap(reader, chunk_size, progress), **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\", line 520, in write\n",
      "    consumer(data)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\", line 515, in consumer\n",
      "    raise _to_error(res)\n",
      "hdfs.util.HdfsError: Permission denied: user=root, access=WRITE, inode=\"/corona_data\":lab09:supergroup:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n",
      "\n",
      "ERROR:hdfs.client:Unable to cleanup temporary folder.\n"
     ]
    },
    {
     "ename": "HdfsError",
     "evalue": "Permission denied: user=root, access=WRITE, inode=\"/corona_data\":lab09:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b352800b902a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 폴더 전송\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/corona_data/loc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data/corona_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m           \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unable to cleanup temporary folder.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mn_threads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfpath_tuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m           \u001b[0m_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_upload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36m_upload\u001b[0;34m(_path_tuple)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_local_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_temp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# First, we gather information about remote paths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, hdfs_path, data, overwrite, permission, blocksize, replication, buffersize, append, encoding)\u001b[0m\n\u001b[1;32m    518\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mAsyncWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m       \u001b[0mconsumer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m   def upload(self, hdfs_path, local_path, n_threads=1, temp_dir=None,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mconsumer\u001b[0;34m(_data)\u001b[0m\n\u001b[1;32m    513\u001b[0m       )\n\u001b[1;32m    514\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_to_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHdfsError\u001b[0m: Permission denied: user=root, access=WRITE, inode=\"/corona_data\":lab09:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n"
     ]
    }
   ],
   "source": [
    "# 폴더 전송\n",
    "# aws 서버에서 진행하고 있기 때문에 권한이 없으면 오류가 발생한다. 밑에서도 계속 발생할 예정임\n",
    "client.upload('/corona_data/loc', './data/corona_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925af9e",
   "metadata": {},
   "source": [
    "## hdfs에 수정하기\n",
    "- hdfs.client.write (파일명, append = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f64f640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "Permission denied: user=root, access=WRITE, inode=\"/rdd/score.txt\":lab09:supergroup:-rw-r--r--\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:352)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1877)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:101)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2921)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:837)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:521)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-05f87cdab0ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/rdd/score.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'최연성 장고 100'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, hdfs_path, data, overwrite, permission, blocksize, replication, buffersize, append, encoding)\u001b[0m\n\u001b[1;32m    518\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mAsyncWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m       \u001b[0mconsumer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m   def upload(self, hdfs_path, local_path, n_threads=1, temp_dir=None,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mconsumer\u001b[0;34m(_data)\u001b[0m\n\u001b[1;32m    513\u001b[0m       )\n\u001b[1;32m    514\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_to_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHdfsError\u001b[0m: Permission denied: user=root, access=WRITE, inode=\"/rdd/score.txt\":lab09:supergroup:-rw-r--r--\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:352)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1877)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:101)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2921)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:837)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:521)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\n"
     ]
    }
   ],
   "source": [
    "client.write('/rdd/score.txt', '최연성 장고 100'.encode('UTF-8'), append = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb095c70",
   "metadata": {},
   "source": [
    "## hdfs 권한 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9826da16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "Permission denied. user=root is not the owner of inode=/corona_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c15edf9b4374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_permission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/corona_data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'777'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mset_permission\u001b[0;34m(self, hdfs_path, permission)\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0;34m'Changing permissions of %r to %r.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     )\n\u001b[0;32m--> 976\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_permission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermission\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodification_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mapi_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    116\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'RetriableException'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'StandbyException'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHdfsError\u001b[0m: Permission denied. user=root is not the owner of inode=/corona_data"
     ]
    }
   ],
   "source": [
    "client.set_permission('/corona_data/', '777')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1672dc",
   "metadata": {},
   "source": [
    "## hdfs 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8cd2e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete('/corona_data/loc/sidd_area.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839837d",
   "metadata": {},
   "source": [
    "### 공공데이터 REST_API 통해서 데이터 수집 후 hdfs에 저장\n",
    "\n",
    "1. 일간 코로나 발생 데이터(전국)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63d843",
   "metadata": {},
   "source": [
    "## REST_API로 데이터를 호출해 HDFS에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f6aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeRestApi(method, url, headers, params):  \n",
    "    \n",
    "    # raise Exception('응답코드 : 500')  # 예외 테스트\n",
    "    # err_num = 10/0 # 예외 테스트\n",
    "    if method == \"get\":\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "    else:\n",
    "        res = requests.post(url, data=params, headers=headers)\n",
    "\n",
    "    if res == None or res.status_code != 200:\n",
    "        raise Exception('응답코드 : ' + str(res.status_code))\n",
    "       \n",
    "    return res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3de26",
   "metadata": {},
   "source": [
    "### 기준일자 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ba1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_std_day(befor_day):\n",
    "    x = dt.datetime.now() - dt.timedelta(befor_day)\n",
    "    year = x.year\n",
    "    month = x.month if x.month >= 10 else '0'+ str(x.month)\n",
    "    day = x.day if x.day >= 10 else '0'+ str(x.day)  \n",
    "    return str(year)+ '-' +str(month)+ '-' +str(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb79c071",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-24baa2ec9c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcal_std_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-b200cef5a992>\u001b[0m in \u001b[0;36mcal_std_day\u001b[0;34m(befor_day)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcal_std_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefor_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefor_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmonth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dt' is not defined"
     ]
    }
   ],
   "source": [
    "cal_std_day(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496c812",
   "metadata": {},
   "source": [
    "### logger\n",
    "- log 를 기록해주는 객체(관련 패키지 존재 : logging)\n",
    "- 외부 연결 시 오류 상태 확인 위해서 사용자 정의 log 사용 가능해야 한다.\n",
    "- restapi 는 외부 서버 연결이므로 log 기록\n",
    "- 로그 관련 디렉터리 \n",
    "    1. 로그 관련 패키지 import(import logging)\n",
    "    2. 로그 관리 객체 logger 생성(logging.getLogger('로거이름'))\n",
    "    3. 로그 파일 핸들러 생성(logging.FileHandler('로그파일명.log))\n",
    "    4. 로거에 파일핸들러 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba9ddc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "565313d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_logger = logging.getLogger('conrona_api')\n",
    "f_handler = logging.FileHandler('./log/rest_api/'+cal_std_day(0)+'log')\n",
    "co_logger.addHandler(f_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6e7b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:conrona_api:corona_patient_2023-12-22.json 다운로드 실패\n"
     ]
    }
   ],
   "source": [
    "# 이렇게 문자열로 찍을 수도 있음\n",
    "# json양식으로 log를 저장하면 다음에 읽어와서 처리하기도 편하다.\n",
    "co_logger.error('corona_patient_'+cal_std_day(0)+'.json 다운로드 실패')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc4080",
   "metadata": {},
   "source": [
    "- 위에서 강제 발생시킨 에러에 대한 log 는 ./log/rest_api/'+cal_std_day(0)+'log 로그 파일에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4dda89",
   "metadata": {},
   "source": [
    "### api 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22ce26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://apis.data.go.kr/1352000/ODMS_COVID_04/callCovid04Api'\n",
    "service_key = 'vpA5w%2Frqnqv5OVUvdS1jzdjrdM3lN%2B3Omjbnx64dXdMBMpL7xbLreYIFaigNiexQelZHSsPNMGL3A91tHKF1YQ%3D%3D'\n",
    "file_dir = '/corona_data/patient/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d960f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std_day : 오늘부터 얼머나 이전 데이터인지 나타냄 0 : 당일/ 1: 어제 / 365 : 1년전\n",
    "def create_param(std_day):\n",
    "    return {'serviceKey' : service_key,\n",
    "           'pageNo' : 1,\n",
    "           'numOfRows' : 500,\n",
    "           'apiType' : 'JSON',\n",
    "            'std_day': cal_std_day(std_day)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d55f7c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serviceKey': 'vpA5w%2Frqnqv5OVUvdS1jzdjrdM3lN%2B3Omjbnx64dXdMBMpL7xbLreYIFaigNiexQelZHSsPNMGL3A91tHKF1YQ%3D%3D',\n",
       " 'pageNo': 1,\n",
       " 'numOfRows': 500,\n",
       " 'apiType': 'JSON',\n",
       " 'std_day': '2022-12-22'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_param(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09a5f585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:conrona_api:{\"is_sucess\": \"Fail\", \"type\": \"corona_patient\", \"std_day\": \"2022-12-22\", \"params\": {\"serviceKey\": \"vpA5w%2Frqnqv5OVUvdS1jzdjrdM3lN%2B3Omjbnx64dXdMBMpL7xbLreYIFaigNiexQelZHSsPNMGL3A91tHKF1YQ%3D%3D\", \"pageNo\": 1, \"numOfRows\": 500, \"apiType\": \"JSON\", \"std_day\": \"2022-12-22\"}, \"err_msg\": \"Permission denied: user=root, access=WRITE, inode=\\\"/corona_data\\\":lab09:supergroup:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\\n\\tat java.base/java.security.AccessController.doPrivileged(Native Method)\\n\\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\\n\"}\n",
      "ERROR:conrona_api:{\"is_sucess\": \"Fail\", \"type\": \"corona_patient\", \"std_day\": \"2022-12-21\", \"params\": {\"serviceKey\": \"vpA5w%2Frqnqv5OVUvdS1jzdjrdM3lN%2B3Omjbnx64dXdMBMpL7xbLreYIFaigNiexQelZHSsPNMGL3A91tHKF1YQ%3D%3D\", \"pageNo\": 1, \"numOfRows\": 500, \"apiType\": \"JSON\", \"std_day\": \"2022-12-21\"}, \"err_msg\": \"Permission denied: user=root, access=WRITE, inode=\\\"/corona_data\\\":lab09:supergroup:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:346)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:242)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1927)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1886)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2685)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2625)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:806)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:496)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1213)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1089)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1012)\\n\\tat java.base/java.security.AccessController.doPrivileged(Native Method)\\n\\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3026)\\n\"}\n"
     ]
    }
   ],
   "source": [
    "for i in range(365, 367) :\n",
    "    params = create_param(i) # api 요청 파라미터 생성\n",
    "    log_dict = {\n",
    "        'is_sucess' : 'Fail'\n",
    "        ,'type' : 'corona_patient'\n",
    "        ,'std_day' : params['std_day']\n",
    "        ,'params' : params\n",
    "    }\n",
    "    try:\n",
    "        res = executeRestApi('get',url, {}, params) # 공공데이터 api에서 추출\n",
    "        file_name = 'corona_patient_'+cal_std_day(i)+'.json' # 저장 파일명 생성\n",
    "        client.write(file_dir+file_name,res, encoding = 'utf=8') # hdfs 에 저장\n",
    "    \n",
    "    except Exception as e :\n",
    "        log_dict['err_msg'] = e.__str__() # exception 객체 내 에러메시지를 반환\n",
    "        log_json = json.dumps(log_dict, ensure_ascii=False) # json형태로 기록 \n",
    "        co_logger.error(log_json) # 로그파일에 기록\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03b62d",
   "metadata": {},
   "source": [
    "## 웹크롤링 hdfs 저장\n",
    "\n",
    "- 예방접종현황(백신 접종현황)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaa536d8-d47c-46bc-b078-d32b1a6afc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb02c77d-b710-4e82-b0c5-f6c2091e64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = executeRestApi('get', url, {},{})\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61ec5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf9b3f2b-4a19-4898-8576-4fc3e2880cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#content > div.data_table.tbl_scrl_mini > table > tbody > tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0e5ca91-ddb5-4e2a-afe5-13242df2e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = soup.select('#content > div.data_table.tbl_scrl_mini > table > tbody > tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8c86f08-62cd-42e3-a59c-cbb62c5da29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e34a99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['loc', 'new', 'accumulate', 'rate']\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "612b9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tr in enumerate(trs):\n",
    "    if idx == 0:\n",
    "        continue # idx 0번 데이터는 전국 데이터 제외해야 하므로 continue\n",
    "    tuple_t = []\n",
    "    # 지역 값\n",
    "    loc_th = tr.find('th')\n",
    "    # 접종관련 데이터들\n",
    "    tds = tr.select('td')\n",
    "    # 지역 이름을 추가\n",
    "    tuple_t.append(loc_th.text)\n",
    "    # 접종관련 데이터 추가\n",
    "    tuple_t.append(tds[0].text.replace(' ','').replace('\\r\\n', '').replace(',',''))\n",
    "    tuple_t.append(tds[1].text.replace(' ','').replace('\\r\\n', '').replace(',',''))\n",
    "    tuple_t.append(tds[2].text.replace(' ','').replace('\\r\\n', '').replace(',',''))\n",
    "    data.append(dict(zip(cols, tuple_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac3bd566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'desc': '지역별 코로나 예방접종 인구 현황',\n",
       "  'cols': {'loc': '지역', 'new': '신규접종', 'accumulate': '누적접종', 'rate': '접종률'},\n",
       "  'stdDay': '2023-12-22'},\n",
       " 'data': [{'loc': '서울', 'v1': '1289', 'v2': '686841', 'v3': '39.6%'},\n",
       "  {'loc': '부산', 'v1': '396', 'v2': '272263', 'v3': '36.3%'},\n",
       "  {'loc': '대구', 'v1': '305', 'v2': '150341', 'v3': '32%'},\n",
       "  {'loc': '인천', 'v1': '445', 'v2': '198337', 'v3': '39.8%'},\n",
       "  {'loc': '광주', 'v1': '124', 'v2': '109590', 'v3': '46.5%'},\n",
       "  {'loc': '대전', 'v1': '141', 'v2': '103662', 'v3': '42.1%'},\n",
       "  {'loc': '울산', 'v1': '179', 'v2': '65936', 'v3': '37.3%'},\n",
       "  {'loc': '세종', 'v1': '43', 'v2': '18264', 'v3': '42.7%'},\n",
       "  {'loc': '경기', 'v1': '1159', 'v2': '861348', 'v3': '40.5%'},\n",
       "  {'loc': '강원', 'v1': '349', 'v2': '141478', 'v3': '38.5%'},\n",
       "  {'loc': '충북', 'v1': '318', 'v2': '142307', 'v3': '42.5%'},\n",
       "  {'loc': '충남', 'v1': '183', 'v2': '189657', 'v3': '41.4%'},\n",
       "  {'loc': '전북', 'v1': '245', 'v2': '196850', 'v3': '46.2%'},\n",
       "  {'loc': '전남', 'v1': '272', 'v2': '228066', 'v3': '48%'},\n",
       "  {'loc': '경북', 'v1': '593', 'v2': '236704', 'v3': '37.3%'},\n",
       "  {'loc': '경남', 'v1': '570', 'v2': '247665', 'v3': '36.8%'},\n",
       "  {'loc': '제주', 'v1': '55', 'v2': '45513', 'v3': '37.5%'}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json 에 포함시킬 meta data 구성\n",
    "res = {\n",
    "    'meta':{\n",
    "        'desc':'지역별 코로나 예방접종 인구 현황',\n",
    "        'cols':{\n",
    "                'loc':'지역'\n",
    "                ,'new':'신규접종'\n",
    "                ,'accumulate':'누적접종' \n",
    "               ,'rate' : '접종률'\n",
    "                },\n",
    "        'stdDay' : cal_std_day(0)\n",
    "    },\n",
    "    'data' : data\n",
    "}\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30cb3187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-abb2d7bb3cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/corona_data/vaccine/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'corona_vaccine_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcal_std_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf=8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "file_dir = '/corona_data/vaccine/'\n",
    "file_name = 'corona_vaccine_'+cal_std_day(1)+'.json'\n",
    "client.write(file_dir+file_name, json.dumps(res, ensure_ascii = False), encoding='utf=8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "691a2923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://localhost:9000/corona_data/vaccine/corona_vaccine_2023-12-21.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-8bff96ea52a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/corona_data/vaccine/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'corona_vaccine_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcal_std_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvaccine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvaccine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://localhost:9000/corona_data/vaccine/corona_vaccine_2023-12-21.json"
     ]
    }
   ],
   "source": [
    "file_dir = '/corona_data/vaccine/'\n",
    "file_name = 'corona_vaccine_'+cal_std_day(1)+'.json'\n",
    "vaccine = spark.read.json(file_dir+file_name)\n",
    "vaccine.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c3fba",
   "metadata": {},
   "source": [
    "### 위 크롤링된 데이터가 전 연령에 대한 백신 관련 데이터가 아니므로 다른 파일 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1ad95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df40febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_vac = pd.read_csv('./data/corona_data/코로나백신접종완료_2022_12.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcb3390f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>제주</td>\n",
       "      <td>591058</td>\n",
       "      <td>585437.0</td>\n",
       "      <td>439106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서울</td>\n",
       "      <td>8335298</td>\n",
       "      <td>8257768.0</td>\n",
       "      <td>6076429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>부산</td>\n",
       "      <td>2879900</td>\n",
       "      <td>2851362.0</td>\n",
       "      <td>2140773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대구</td>\n",
       "      <td>2018604</td>\n",
       "      <td>1995783.0</td>\n",
       "      <td>1405163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>광주</td>\n",
       "      <td>1261981</td>\n",
       "      <td>1250568.0</td>\n",
       "      <td>968947.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LOC       V1         V2         V3\n",
       "0  제주   591058   585437.0   439106.0\n",
       "1  서울  8335298  8257768.0  6076429.0\n",
       "2  부산  2879900  2851362.0  2140773.0\n",
       "3  대구  2018604  1995783.0  1405163.0\n",
       "4  광주  1261981  1250568.0   968947.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9668f92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광주'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = fin_vac.loc[4]\n",
    "tmp.LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43832f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['loc', 'v1', 'v2', 'v3']\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94704338",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in fin_vac.index:\n",
    "    tuple_t = []\n",
    "    tmp = fin_vac.loc[idx]\n",
    "    tuple_t.append(str(tmp.LOC))\n",
    "    tuple_t.append(str(tmp.V1))\n",
    "    tuple_t.append(str(tmp.V2))\n",
    "    tuple_t.append(str(tmp.V3))\n",
    "    data.append(dict(zip(cols, tuple_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "977e39b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loc': '제주', 'v1': '591058', 'v2': '585437.0', 'v3': '439106.0'},\n",
       " {'loc': '서울', 'v1': '8335298', 'v2': '8257768.0', 'v3': '6076429.0'},\n",
       " {'loc': '부산', 'v1': '2879900', 'v2': '2851362.0', 'v3': '2140773.0'},\n",
       " {'loc': '대구', 'v1': '2018604', 'v2': '1995783.0', 'v3': '1405163.0'},\n",
       " {'loc': '광주', 'v1': '1261981', 'v2': '1250568.0', 'v3': '968947.0'},\n",
       " {'loc': '인천', 'v1': '2574233', 'v2': '2550081.0', 'v3': '1915266.0'},\n",
       " {'loc': '대전', 'v1': '1245473', 'v2': '1233376.0', 'v3': '909102.0'},\n",
       " {'loc': '경기', 'v1': '11837174', 'v2': '11723258.0', 'v3': '8707827.0'},\n",
       " {'loc': '세종', 'v1': '300568', 'v2': '296859.0', 'v3': '211095.0'},\n",
       " {'loc': '경남', 'v1': '2882823', 'v2': '2853755.0', 'v3': '2145652.0'},\n",
       " {'loc': '경북', 'v1': '2293225', 'v2': '2269593.0', 'v3': '1727188.0'},\n",
       " {'loc': '전남', 'v1': '1655633', 'v2': '1641348.0', 'v3': '1365245.0'},\n",
       " {'loc': '전북', 'v1': '1593950', 'v2': '1581091.0', 'v3': '1285767.0'},\n",
       " {'loc': '충남', 'v1': '1902008', 'v2': '1884584.0', 'v3': '1479736.0'},\n",
       " {'loc': '충북', 'v1': '1430877', 'v2': '1418444.0', 'v3': '1108508.0'},\n",
       " {'loc': '강원', 'v1': '1352085', 'v2': '1340870.0', 'v3': '1065143.0'},\n",
       " {'loc': '울산', 'v1': '968160', 'v2': '958920.0', 'v3': '720521.0'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fe3ce48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'desc': '지역별 코로나 예방접종 인구 현황',\n",
       "  'cols': {'loc': '지역', 'V1': '1차 접종 누적', 'V2': '2차 접종 누적', 'V3': '3차 접종 누적'},\n",
       "  'stdDay': '2022-12-22'},\n",
       " 'data': [{'loc': '서울', 'v1': '1289', 'v2': '686841', 'v3': '39.6%'},\n",
       "  {'loc': '부산', 'v1': '396', 'v2': '272263', 'v3': '36.3%'},\n",
       "  {'loc': '대구', 'v1': '305', 'v2': '150341', 'v3': '32%'},\n",
       "  {'loc': '인천', 'v1': '445', 'v2': '198337', 'v3': '39.8%'},\n",
       "  {'loc': '광주', 'v1': '124', 'v2': '109590', 'v3': '46.5%'},\n",
       "  {'loc': '대전', 'v1': '141', 'v2': '103662', 'v3': '42.1%'},\n",
       "  {'loc': '울산', 'v1': '179', 'v2': '65936', 'v3': '37.3%'},\n",
       "  {'loc': '세종', 'v1': '43', 'v2': '18264', 'v3': '42.7%'},\n",
       "  {'loc': '경기', 'v1': '1159', 'v2': '861348', 'v3': '40.5%'},\n",
       "  {'loc': '강원', 'v1': '349', 'v2': '141478', 'v3': '38.5%'},\n",
       "  {'loc': '충북', 'v1': '318', 'v2': '142307', 'v3': '42.5%'},\n",
       "  {'loc': '충남', 'v1': '183', 'v2': '189657', 'v3': '41.4%'},\n",
       "  {'loc': '전북', 'v1': '245', 'v2': '196850', 'v3': '46.2%'},\n",
       "  {'loc': '전남', 'v1': '272', 'v2': '228066', 'v3': '48%'},\n",
       "  {'loc': '경북', 'v1': '593', 'v2': '236704', 'v3': '37.3%'},\n",
       "  {'loc': '경남', 'v1': '570', 'v2': '247665', 'v3': '36.8%'},\n",
       "  {'loc': '제주', 'v1': '55', 'v2': '45513', 'v3': '37.5%'}]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json 에 포함시킬 meta data 구성\n",
    "res = {\n",
    "    'meta':{\n",
    "        'desc':'지역별 코로나 예방접종 인구 현황',\n",
    "        'cols':{\n",
    "                'loc':'지역'\n",
    "                ,'V1':'1차 접종 누적'\n",
    "                ,'V2':'2차 접종 누적' \n",
    "                ,'V3':'3차 접종 누적'\n",
    "                },\n",
    "        'stdDay' : cal_std_day(365)\n",
    "    },\n",
    "    'data' : data\n",
    "}\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94537e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/corona_data/vaccine/'\n",
    "file_name = 'corona_vaccine_'+cal_std_day(365)+'.json'\n",
    "client.write(file_dir+file_name, json.dumps(res, ensure_ascii = False), encoding='utf=8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/corona_data/vaccine/'\n",
    "file_name = 'corona_vaccine_'+cal_std_day(365)+'.json'\n",
    "vaccine = spark.read.json(file_dir+file_name)\n",
    "vaccine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d1072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
