### 퍼셉트론(Perceptron)

- 인공 신경망의 한 종류로, 단층 인공 신경망을 나타내는 가장 간단한 형태 

- 퍼셉트론은 이진 분류 문제를 해결하는 데 사용되며, 두 개의 `클래스(노드) 중 하나로 데이터 포인트를 분류하는 데 초점`을 맞춤. 선형회귀식을 사용한다.


- 주요 구성 요소

1. **입력 (Input):** 퍼셉트론은 입력 특성(또는 변수)을 받습니다. 이러한 입력 특성은 실수 값이며, 각 특성은 데이터 포인트의 속성을 나타냅니다.

2. **가중치 (Weight):** 입력 특성마다 가중치가 할당됩니다. 가중치는 입력 신호에 대한 중요도를 나타내며, 학습 과정에서 조절됩니다. 가중치는 각 입력과 연결되어 있으며, 각 입력에 대한 가중치는 해당 입력의 중요성을 조절합니다.
x1 x2 를 통해 y 를 도출해내는 선형방정식이다.

1. **활성화 함수 (Activation Function):** 퍼셉트론의 출력을 결정하는 함수입니다. 가중치와 입력 특성의 선형 조합(가중합)을 계산하고 이 결과를 활성화 함수에 적용하여 최종 출력을 생성합니다. 가장 간단한 형태의 활성화 함수는 계단 함수(Step Function)이며, 이진 분류 문제에 사용됩니다.

2. **바이어스 (Bias):** 퍼셉트론은 바이어스 항을 가질 수 있습니다. 이것은 입력과 가중치의 곱의 합에 더해지는 상수입니다. 바이어스는 모델의 편향을 나타내며, 퍼셉트론의 결정 경계를 조절하는 데 도움이 됩니다.

퍼셉트론은 학습 알고리즘을 통해 가중치를 조정하여 올바른 분류 결정 경계를 학습합니다. 학습 알고리즘은 예측과 실제 결과를 비교하여 `오류를 최소화하고 가중치를 업데이트하는 과정을 반복`합니다. 이러한 학습 알고리즘은 데이터가 선형 분리 가능한 경우에 수렴하며, 선형 분리 불가능한 경우에는 수렴하지 않을 수 있습니다.

### 다층 퍼셉트론(Multi-Layer Perceptron)

- and, nand 등은 쉽게 계산할 수 있으나 선형 방정식에서 해결할 수 없는 XOR을 어떻게 적용해야 하는 것에 대해서 문제가 발생함.

- 퍼셉트론은 가장 간단한 형태의 신경망이며, 여러 개의 퍼셉트론을 결합하여 `다층 퍼셉트론`(Multi-Layer Perceptron, MLP)을 만들 수 있습니다. ->> 현재는 신경망이라는 용어로 사용함.

- `다층의 개념`에서 XOR 의 해결점을 찾아내었다. 논리적으로 평면을 휘게 만들어(공간을 왜곡) 두 영역을 가로지르는 선을 찾아낸다. 

층을 더 추가해서 입력층, (은닉층), 출력층 이런 식으로 구성

   - 입력층 (Input Layer): 두 개의 입력 노드로부터 입력값을 받습니다.

   - 은닉층 (Hidden Layer): 하나 이상의 은닉층이 있으며, 은닉층은 비선형 활성화 함수를 사용하여 입력값을 변환합니다.

   - 출력층 (Output Layer): 하나의 출력 노드로서 XOR 연산 결과를 출력합니다.

-> MLP는 복잡한 비선형 문제를 해결하는 데 사용


### 역전파(Backpropagation)

인공 신경망의 학습 알고리즘 중 하나로, 가중치와 편향을 조절하여 모델을 훈련시키는 과정을 의미합니다. 

역전파는 주로 다층 퍼셉트론(MLP)과 같은 심층 신경망을 학습하는 데 사용됩니다. 

역전파 알고리즘은 딥러닝 모델의 학습을 가능하게 하는 핵심 알고리즘 중 하나로, 신경망이 복잡한 함수를 근사하고 데이터를 학습하는 데 사용

역전파를 통해 모델은 입력과 출력 간의 관계를 학습하고, 이를 통해 다양한 작업에 대한 예측을 수행할 수 있습니다.

- 주요 개념과 원리

1. **순전파 (Forward Propagation):** 역전파 알고리즘은 먼저 입력 데이터를 모델에 제공하고 순전파를 수행합니다. 입력 데이터는 각 계층의 가중치와 편향을 통과하여 출력을 생성합니다.

2. **오차 계산:** `순전파`로 생성된 출력과 실제 정답(타깃)과의 오차를 계산합니다. 일반적으로 평균 제곱 오차(Mean Squared Error, MSE)나 교차 엔트로피 손실(Cross-Entropy Loss) 등의 손실 함수를 사용하여 오차를 측정합니다.

3. **역전파:** 역전파 알고리즘의 `핵심은 오차를 거꾸로 전파하는 과정`입니다. 오차를 출력층에서부터 입력층 방향으로 전파하여 `각 가중치와 편향을 업데이트`합니다. 이때, `경사 하강법(Gradient Descent)을 사용`하여 가중치와 편향을 최적화합니다.

4. **가중치 업데이트:** 오차의 그래디언트(기울기)를 계산하고, 이를 사용하여 가중치와 편향을 업데이트합니다. 역전파는 오차를 최소화하기 위해 경사 하강법을 활용하는데, 이때 학습률(learning rate)과 같은 하이퍼파라미터를 조절합니다.

5. **반복:** 위의 과정을 여러 번 반복하여 모델의 가중치와 편향을 조절하고 오차를 최소화합니다. 이 반복 과정은 일반적으로 미니배치 경사 하강법을 사용하여 수행됩니다.

6. **수렴:** 모델이 수렴할 때까지 반복적으로 학습을 진행합니다. 모델의 오차가 더 이상 감소하지 않거나, 정해진 반복 횟수를 초과하면 학습을 종료합니다.


7. **손실 함수 (Loss Function):** 

- 손실 함수는 모델의 출력과 `실제 정답(또는 타깃) 간의 차이를 측정`하는 함수입니다.
  
- 모델이 얼마나 잘 예측하는지를 평가하고 모델의 성능을 측정하기 위해 사용
  
- 손실 함수의 값이 작을수록 모델의 예측이 정확하다는 것을 의미합니다.

- 회귀 문제의 경우, 일반적으로 평균 제곱 오차(Mean Squared Error, MSE) 또는 평균 절대 오차(Mean Absolute Error, MAE)를 사용합니다.


8. **오차 역전파 (Backpropagation):**

- 오차 역전파는 신경망 학습 알고리즘 중 하나로, 손실 함수의 기울기를 계산하고 이를 사용하여 가중치 및 편향을 업데이트하는 과정을 의미합니다.

- 역전파는 `기울기 하강법`(Gradient Descent)과 함께 사용되어 가중치 조정을 통해 모델을 학습합니다.

- 역전파 과정은 다음과 같이 진행됩니다:

- 손실 함수를 사용하여 예측과 실제 정답 사이의 오차를 계산합니다.

- 손실 함수의 그래디언트를 역방향으로 전파하면서 각 가중치와 편향에 대한 그래디언트를 계산합니다.

- 계산된 그래디언트를 사용하여 가중치와 편향을 업데이트합니다. 이때, 학습률과 같은 하이퍼파라미터를 사용하여 업데이트의 크기를 조절합니다.

- 위 과정을 반복 손실 함수의 값이 최소가 되도록 모델을 학습합니다.

9. **최적화(Optimization) 도구:**

- 머신 러닝과 딥러닝 모델의 학습을 효율적으로 수행하고 모델의 성능을 향상시키기 위한 알고리즘 및 방법론입니다. 

10. **에포크 (Epoch):**

- 에포크는 전체 학습 데이터셋을 한 번 순회하는 것을 의미합니다. 즉, 모델이 전체 학습 데이터를 한 번 학습한 주기를 에포크라고 합니다.
  
- 에포크가 증가할수록 모델은 더 많은 학습 과정을 거치게 되며, 학습 데이터에 대한 오차를 줄여나갑니다.

- 에포크를 증가시키면 모델이 학습 데이터에 더 적합해질 수 있지만, `과적합(Overfitting) 문제`가 발생할 수 있습니다.

11.  **배치 크기 (Batch Size):**

- 배치 크기는 `각 에포크에서 모델이 처리하는 데이터의 묶음`을 의미합니다. `즉, 한 번에 처리되는 데이터 포인트의 수`를 나타냅니다.
  
- 작은 배치 크기를 사용하면 더 빠른 학습이 가능하며, 메모리 요구사항이 낮아집니다. 그러나 학습 과정이 불안정할 수 있습니다.
  
- 큰 배치 크기를 사용하면 학습이 안정적이지만 더 많은 메모리가 필요하며, 학습 속도가 느려질 수 있습니다.
