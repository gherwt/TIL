# 규제가 있는 선형 모델

- 과대적합을 감소시키는 것이 아주 중요하고, 그 과대적합을 줄일 수 있는 방법을 강구해야 한다.

- 그 방법 중 loss에 새로운 term을 추가하여서 모델 weight에 대한 규제를 가하는 것이다.

## Lasso Regression (라쏘 회귀)

- Lasso(라쏘)회귀는 선형 회귀의 규제된 버전이다.

- 비용 함수에 `L1` term(가중치에 `절대값`을 적용한 형태)을 사용하여서 weight에 규제를 한다. 가중치에 절댓값을 활용

- w-> 0으로 만들어 차원 축소의 효과가 있다.

- MSE 에서만 W 규제 -> MSE + W 둘다 규제

```py
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1) # 알파값에 따라 규제를 한다.
lasso_reg.fit(x,y)
lasso_reg.predict([[1.5]])
```

## Ridge Regression (릿지 회귀)

- Ridge(릿지)회귀는 선형 회귀의 규제된 버전이다.

- 비용 함수에 `L2` term(가중치에 `제곱`을 한 형태)을 사용하여 weight에 규제를 한다.

- w 를 0에 수렵한 값으로 나타낸다. 릿지보다 많이 사용된다.

```py
# 안드레 루이 숄레스키가 발견한 행렬 분해(matrix factorization) 사용
# 숄레스키 분해의 장점은 성능이다. 원래 ridge의 solver default값은 'auto'이며 희소 행렬이나 특이 행렬이 아니면 'cholesky'가 된다.
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=0.1,solver='cholesky')
ridge_reg.fit(x,y)
ridge_reg.predict([[1.5]])
>array([[0.29217567]])
```

## Elastic Net Regression (엘라스틱넷 회귀)

- Elastic Net(엘라스틱넷)은 `릿지 회귀와 라쏘 회귀를 합성`한 모델이다.

- 규제 term은 릿지회귀의 규제term과 단순히 더해서 사용하고, 혼합 비율 `r을 조절해서 사용`한다.

- r=0이면, 엘라스틱 넷은 => 릿지 회귀

- r=1이면, 엘라스틱 넷은 => 라쏘 회귀

```py
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1,l1_ratio=0.5)
elastic_net.fit(x,y)
elastic_net.predict([[1.5]])
```

## Early Stopping (조기 종료)

- 경사 하강법과 같은 반복적인 학습 알고리즘을 규제하는 것은 실제 비용함수에 규제항을 추가하는 것 뿐만 아니라, 검증 `에러가 훈련 에러에 비해 치솟게 되면 바로 훈련을 중지`시키는 방법도 있다. 이를 `early stopping`이라 한다.

```py
from sklearn.base import clone # 모델을 계속해서 복사
from sklearn.preprocessing import PolynomialFeatures,StandardScaler # 전처리 모델
from sklearn.linear_model import SGDRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error

x = 3 * np.random.rand(data_num,1) - 1
y = 0.2 * x**2 + np.random.randn(100,1)

poly_scaler = Pipeline([
                        ("poly_features",PolynomialFeatures(degree=90,include_bias=False)),
                        ('std_scaler',StandardScaler())
])

x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2)
x_train_poly_scaled = poly_scaler.fit_transform(x_train)
x_val_poly_scaled = poly_scaler.transform(x_val)

# warm_start=True 이면 fit 메서드가 호출될 때 처음부터 다시 하지 않고 이전 모델 파라미터에서 훈련 이어짐
# penalty : {‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’
# n_iter_no_change : Number of iterations with no improvement to wait before stopping fitting
# 'constant' : eta = eta0
# 'optimal' : eta = 1.0 / (alpha * (t + t0))
# 'invscaling' : eta = eta0 / pow(t, power_t)
# 'adaptive' : eta = eta0, as long as the training keeps decreasing
sgd_reg = SGDRegressor(n_iter_no_change=1,warm_start=True,penalty=None,
                       learning_rate='constant',eta0=0.0005)
SGDRegressor()
minimum_val_error = float('inf')
best_epoch = None
best_model = None
for epoch in range(1000):
  sgd_reg.fit(x_train_poly_scaled,y_train.ravel())
  y_val_predict = sgd_reg.predict(x_val_poly_scaled)
  val_error = mean_squared_error(y_val,y_val_predict)
  if val_error<minimum_val_error:
    minimum_val_error = val_error
    best_epoch = epoch
    best_model = clone(sgd_reg)
print('best_epoch : ',best_epoch)
print('best_model : ',best_model)
```