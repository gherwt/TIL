# 다항회귀

- `비선형성`을 띄는 데이터도 선형 모델을 활용하여서 학습시킬 수 있다.

- 기존 특성에다가 `log, exp, 제곱` 등과 같은 basis function(특성공학기법)을 적용하여, 확장된 특성을 포함한 형태로 변형한 뒤 학습시키는 것을 다항 회귀 기법이라고 한다.

- 위와 같은 예시를 2차 방정식으로 간단하게 들어 보겠다.

```py
import numpy as np
data_num = 1000
x = 3 * np.random.rand(data_num,1) - 1 # 균일 분포
y = 0.2 * (x**2) + np.random.randn(1000,1) # 제곱 -> 2차 방정식으로 만들어준다.

from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2,include_bias=False)
x_poly = poly_features.fit_transform(x)
print(x[0])
print(x_poly[0])

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x_poly,y)
print(lin_reg.intercept_,lin_reg.coef_)
```

- 특성이 여러 개 일 때 다항 회귀는 이 특성 사이의 관계를 찾을 수 있습니다. (`PolynomialFeatures`를 통해서 주어진 차수까지 특성 간의 모든 교차항을 추가할 수 있기 때문)

훈련 세트와 검증 세트의 모델 성능을 살펴 보는 것. (모델 과적합을 가시적으로 확인 하는 법)

```py
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def plot_learning_curves(model,x,y):
  x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2)
  train_errors,val_errors = [],[]
  for num in range(1,len(x_train)):
    model.fit(x_train[:num],y_train[:num])
    y_train_predict = model.predict(x_train[:num])
    y_val_predict = model.predict(x_val)
    train_errors.append(mean_squared_error(y_train[:num],y_train_predict))
    val_errors.append(mean_squared_error(y_val,y_val_predict))
  plt.plot(np.sqrt(train_errors),'r-+',linewidth=2,label='train_set')
  plt.plot(np.sqrt(val_errors),'b-',linewidth=3,label='val_set')   
  plt.legend()
  plt.show()
```

```py
from sklearn.pipeline import Pipeline
data_num = 100
x = 3 * np.random.rand(data_num,1) - 1
y = 0.2 * x**2 + np.random.randn(100,1)
polynomial_regression = Pipeline([ # 파이프라인으로 묶어준다. 생산성있는 모델을 구성할 수 있다.
  ("poly_features",PolynomialFeatures(degree=4,include_bias=False)), 
  ("lin_reg",LinearRegression())
  ])
plot_learning_curves(polynomial_regression,x,y)
```