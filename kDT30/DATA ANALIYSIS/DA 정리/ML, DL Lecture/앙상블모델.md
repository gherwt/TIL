### 앙상블 모형, 랜덤포레스트

- 어떠한 한 현상에 대한 답을 얻는다고 가정해보자, 많은 경우에 한 명의 전문가보다 `여려 명의 일반인들의 의견이 더 나은 경우`가 있다.

- 위 예제와 비슷하게, 하나의 좋은 모형(회귀,분류)으로부터 예측을 하는 것보다 `여러 개의 모형으로부터 예측을 수집하는 것이 더 좋은 예측`을 할 수 있다. 여러 개의 예측을 종합하여 결과를 도출하는 것이다.

- 이러한 `여러 개의 모형을 앙상블`이라고 부르고, 여러 개의 모형을 `조화롭게 학습시키는 것을 앙상블 학습`이라고 한다.

- 이전에 학습한 결정 트리 모형이 하나가 아니라, 훈련 세트를 무작위로 다른 서브셋으로 만들어서 `결정 트리 분류기`를 만들고, 많은 모형들 중에서 `가장 많은 선택을 받은 클래스를 예측하는 앙상블 모형을 랜덤포레스트`라고 한다.

- 앙상블 내에 랜덤포레스트가 포함된다고 볼 수도 있다.

- 오늘날의 랜덤포레스트 모델은 가장 강력한 머신러닝 알고리즘 하나이다. 그리고 머신러닝 대회에서 우승하는 솔루션들은 대부분 앙상블 방법을 사용하여서 최고 성능을 내고 있다.

### 투표 기반 분류기

- 하나의 데이터셋을 여러종류의 분류기(로지스틱, svm, 랜덤포레스트, KNN 등등)들로 훈련시켰다고 가정해보자.
위에서 언급한대로 하나의 좋은 모델을 사용하는 것보다, 여러 종류의 분류기들이 가장 많이 예측한 클래스를 예측하는 것이 더 좋은 분류기를 만드는 매우 간단한 방법이다.

- 이렇게 다수결의 투표로 정해지는 분류기를 `hard voting`(직접 투표) 분류기라고 한다.

- 놀랍게도 위 모델 중 가장 성능이 좋은 모델의 정확도보다 다수결을 통해 예측한 앙상블 모델의 성능이 높은 경우가 많다.

- 이렇게 랜덤 추측보다 조금 더 높은 성능을 내는 weak learner(약한 학습기) 가 충분히 많고 다양하다면 strong learner(강한 학습기)가 될 수 있다.

### 큰수의 법칙 (약한 학습기?, 강한 학습기?)

- 먼저, 50:50의 동전이 아니라, 51:49의 불균형하게 앞면과 뒷면이 나오는 동전이 있다고 가정을 해보자.

- 이 동전을 1,000번을 던진다면 거의 앞면 510번과 뒷면 490번이 나올 것이다. -> 상식적인 결과

- 수학적으로 1,000번을 던졌을 때 앞면이 더 많게 나오는 확률은 거의 `75% 정도` 된다. 앞면일 확률이 510 번이다.

- 수학적으로 10,000번을 던졌을 때 앞면이 더 많게 나오는 확률은 거의 97% 정도 된다. 통계적인 계산을 통해 도출된 결과

- 위 수학적 계산은 이항분포의 확률 질량 함수로 계산 가능하다.
전부 다 독립이기 때문에 가능한 결과이다.
 ex) 1-scipy.stats.binom.cdf(499,1000,0.51) = 0.747

- 위의 내용을 기반으로 우리의 약한 분류기(51%) 1,000개로 앙상블 모형을 구축하고, 가장 많은 클래스를 예측으로 삼는다면 75%, 10,000개로 모형을 만들면 97% 정도의 성능을 낼 수 있다.

- 하지만 위의 과정은 모든 분류기가 완벽하게 독립이고, 모델의 예측 오차에 대해서 상관관계가 없을때만 가능하다.

  - TIP : 앙상블에서 예측기가 가능한 `서로 독립일 때 최고 성능을 발휘`한다. 그래서 가능한 다양한 알고리즘을 사용해서 학습을 하면 다양한 종류의 오차를 만들기 때문에 앙상블 모델의 성능을 높일 수 있다. -> 오차들끼리의 연관성을 줄여야 된다는 말이다.
