# 배깅과 페이스팅

- 앙상블 모형의 좋은 성능을 내기 위해서는 `다양한 종류의 오차`를 만들어야 하고, 그러기 위해서는 다양한 알고리즘을 사용해야 한다고 배웠다.

- 다양한 오차를 만들기위한 다른 하나의 방법으로는 훈련 세트의 서브셋을 `무작위로 구성`하여 모델을 학습시키는 것이 있다. 이를 배깅과 페이스팅이라고 부른다.

- 배깅 : 훈련 세트의 `중복을 허용`하여 샘플링을 하는 방식 (통계학에서는 "부트스트래핑"이라고도 부름) -> 똑같은 모델을 허용하는가 <<< 훈련 세트의 다양화 / 오버피팅을 줄이고 일반화시키는 것이 목적이다.

- 페이스팅 : 훈련 세트의 `중복을 허용하지 않고` 샘플링 하는 방식

- 배깅은 각 예측기가 학습하는 서브셋에 `다양성을 증가`시키므로 페이스팅보다 편향이 조금 더 높다. => 일반화 성능이 줄어든다.

- 하지만 배깅은 예측기들의 상관관계를 줄이므로 앙상블의 `분산을 감소`시킨다.
-> 편향과 분산은 트레이드오프 관계가 있다고 한다.

- `전반적으로 배깅이 더 나은 모델을 만들지만`, 시간과 장비가 좋다면 교차검증으로 배깅과 페이스팅을 둘다 해보면 좋다.

# oob 평가

- 배깅(중복 허용 샘플링)을 하다보면 평균적으로 훈련 샘플의 약 63%정도만 추출되고 나머지 약 37%는 추출되지 않고, 이렇게 `추출되지 않은 샘플들을 oob(out-of-bag, 가방 밖에 있다.)샘플`이라고 부른다.

- 예측기가 훈련되는 동안에는 oob샘플을 사용하지 않으므로, 검증 세트나 교차 검증을 사용하지 않고 `oob샘플만을 가지고 모델 최적화를 위한 평가`를 할 수 있다. 검증데이터를 사용하지 않는다고 보면 된다.
훈련세트를 샘플에 적용해서 정확도를 측정하는 것임.

- 앙상블의 평가는 각 예측기의 oob평가의 평균으로 확인한다.

# 랜덤 패치와 랜덤 서브스페이스

- 위에서는 훈련 샘플을 랜덤 샘플링하여 각 예측기의 오차 다양성을 주었지만, 이번에는 훈련 데이터들의 입력 `특성`들을 무작위로 샘플링하여 예측기를 만들어서 예측기에 대한 오차 다양성을 줄 수 있다.

- 모델, 샘플, 특성 다양하게 중 특성을 선택

- 훈련 데이터들의 입력 특성들을 무작위로 샘플링하는 것을 랜덤 패치 방식과 랜덤 서브스페이스 방식이라고 한다.

- 랜덤 패치 방식 : 훈련 `특성과 샘플을 모두 샘플링`하는 방식
  - ex) (bootstrap=True or False, max_samples<1.0, bootstrap_features=True, max_features<1.0)
  
- 랜덤 서브스페이스 방식 : 훈련 샘플은 모두 사용하고, `특성만 샘플링` 하는 것, 배깅이라는 개념들은 뺴고 진행한다.
  - ex) (bootstrap=False, max_samples=1.0, bootstrap_features=True, max_features<1.0)

# 랜덤 포레스트

- 랜덤포레스트는 일반적으로 배깅방법을 사용한 결정트리 앙상블 모델이다.

- 그래서 BaggingClassifier에 DecisionTreeClassifier를 넣는 대신, `RandomForestClassifier`를 사용할 수 있다. 두 개를 섞어서 사용하지말고 같이 사용하게 끔 만들어준다.

- RandomForestClassifier는 DecisionTreeClassifier와 BaggingClassifier 매개변수 모두 가지고 있다.
매개변수도 어느정도 공통으로 사용하고 있다.

- 랜덤포레스트 모델은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 것이 아니라, `무작위로 선택한 특성들 중에서 최선의 특성을 찾는 방식을 채택`하여 무작위성을 더 가지게 된다. 그리드 서치 x, 오차에 대한 다양성을 추구

- 이를 통해 약간의 편향은 손해보지만, 더욱 다양한 트리를 만들므로 분산을 전체적으로 낮추어서 더 훌륭한 모델을 만들 수 있다.

## 엑스트라 트리

- 랜덤포레스트는 앞에서 말한 것 처럼 각 노드에서 무작위로 특성을 뽑은 다음 최적의 특성과 임계값을 선택한다.

- 하지만 엑스트라 트리는 최적의 특성과 임계값을 찾는것 대신, 후보 특성을 사용해 `무작위로 분할한 다음에 최상을 분할을 선택`한다.

- 이렇게되면 기본적으로 편향이 많은 랜던포레스트보다 더욱 편향이 심해지지만, 분산을 더욱 낮출 수 있게 된다. 트레이드오프 효과

- 트리 알고리즘에서는 모든 노드에서 최적의 특성과 임계값을 고르는데 시간이 많이 들지만, 엑스트라 트리를 사용하면 훈련과 예측속도가 빨라진다.

- 엑스트라 트리는 ExtraTreesClassifier를 이용하면 사용할 수 있다.

- RandomForestClassifier와 ExtraTreesClassifier 중 어떤 것이 더 좋을지는 판단하기 어렵기 때문에, 교차검증을 통해서 서로 비교해보고, 더 나은 모델을 선택하여 그리드 탐색방법을 사용해 하이퍼파라미터 튜닝을 한다.

## 특성 중요도

- 랜덤포레스트는 성능이 좋다는 장점말고, `특성의 상대적 중요도를 측정하기 쉽다`.(트리기반 모델은 특성 중요도 제공, 해석력이 좋다)

- 사이킷런에서는 어떤 특성을 사용한 노드가 평균적으로 불순도를 감소시키는지 확인하여 특성 중요도를 측정하고, 훈련이 끝나고 난 뒤에 특성마다 자동으로 점수를 계산하고 저장한다. 정확도보다 중요 요인들을 확인하고 싶을 때 사용한다.

- 저장된 값은 featureimportances 변수에 저장되어 있다.

## 부스팅

- 부스팅이란, 약한 학습기를 여러 개들을 서로 연결하고 보완해가면서 더욱 강한 학습기를 만드는 앙상블 방법이다.

- 다양한 부스팅 방법들이 있지만, 그중에서 가장 인기있는 아다부스트와 그래디언트 부스팅을 소개하겠다.

### 아다부스트

- 아다부스트의 아이디어는 이전 예측기가 `과소적합(성능을 더 높힐 수 있는 모델)`되었던 훈련 샘플의 `가중치를 더 높이는 것`이다.

- 이 덕분에 새로운 예측기는 학습하기 어려운 샘플에 대해 더욱 잘 예측하게 된다.

- 예를들어보면,아다부스트에서 첫 번째 예측기를 결정트리로 훈련시키고 예측을 했을 때, 잘못 분류된 훈련 샘플에 대해 가중치를 상대적으로 높이고, 두 번째에는 업데이트된 가중치를 통해서 예측의 오분류를 확인하고 가중치를 높일 것인지 낮출 것인지 결정되면서 반복된다.(성능이 안 좋은 모델을 끌어올리는 부스팅 방법이다.) -> 점점 수정하는 방법

- 사이킷런에서 제공하는 아다부스트는 다음과 같이 수행해볼 수 있다.

### 그래디언트 부스팅

- 그래디언트 부스팅은 아다부스팅과 비슷하게 학습 샘플에 대해 `오차를 보정`하면서 순차적으로 예측기를 추가 한다.

- 하지만 차이점은 아다부스트처럼 각 학습 샘플에 대한 가중치를 업데이트 하는 대신, 이전 예측기가 만든 `잔차(residual error, 실제 모델과 예측값의 차이를 줄이기 위한 학습기를 만든다.)`에 새로운 예측기를 학습시키는 것이다.

- 결정트리를 예측기로 활용하여 그래디언트 부스팅의 회귀 문제를 수행해 보겠고, 결정트리와 그래디언트 부스팅을 함께 적용한 이 알고리즘을 보통 그래디언트 부스티드 회귀 트리(Gradient Boosted Regression Tree = `GBRT`)라고 부른다.
