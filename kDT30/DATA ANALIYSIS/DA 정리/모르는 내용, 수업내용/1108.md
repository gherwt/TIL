### LSTM (Long Short Term Memory)

- RNN (Recurrent Neural Network)의 단점 중 하나인 `장기 의존성 문제를 해결`하기 위해 개발된 신경망 아키텍처입니다. 

- `RNN은 직전 데이터만 고려`하므로 긴 기간을 학습하는데 있어서 중요한 데이터를 기억하고 유지하는 것에 어려움이 있었습니다. LSTM은 이러한 문제를 해결하기 위해 디자인된 신경망 구조입니다.

LSTM의 중요한 특징과 구성 요소에 대한 설명:

1. **셸 상태 (Cell State):** LSTM은 두 가지 상태를 유지합니다. 하나는 단기 기억을 나타내는 **셸 상태**이며, 다른 하나는 단기 상태를 나타내는 **은닉 상태**입니다. 셸 상태는 현재 시점의 입력 데이터와 이전 시점의 셸 상태로부터 계산됩니다.

2. **입력 게이트 (Input Gate):** 입력 게이트는 현재 입력 데이터가 셸 상태에 어떻게 추가되는지를 결정합니다. 현재 정보를 얼마나 반영할지를 제어하는 역할을 합니다.

3. **망각 게이트 (Forget Gate):** 망각 게이트는 이전 셸 상태에서 어떤 정보를 삭제할지를 결정합니다. 가중치를 설정하고 이에 따라서 불필요한 기억을 삭제함으로써 중요한 기억에 데이터를 명확한 상태로 유지해준다.

4. **출력 게이트 (Output Gate):** 출력 게이트는 셸 상태를 바탕으로 최종 출력값을 계산합니다. 셸 상태의 정보를 다음 시점의 은닉 상태로 보내거나 출력으로 사용하는 역할을 합니다.(장기/단기 기억 데이터로 변환)

5. **은닉 상태 (Hidden State):** 은닉 상태는 LSTM 층의 출력이며, 현재 시점에서의 은닉 상태는 입력 게이트와 출력 게이트의 `가중치`에 따라 결정됩니다. 은닉 상태는 다음 시점의 LSTM 층으로 전달되거나 출력으로 사용됩니다.

LSTM은 장기 의존성 문제를 해결하고 긴 시퀀스를 학습할 수 있도록 고안되었습니다. 장기 기억을 유지하면서 중요한 정보를 강조하고 불필요한 정보를 제거함으로써 효과적으로 시퀀스 데이터를 처리합니다. LSTM은 다양한 자연어 처리 및 시계열 예측 작업에서 매우 유용하게 활용된다.


### 사용 코드 중에서 실습 때 오류로 인해 검색한 코드들

### 1. 텍스트 데이터 전처리 

- 경로 설정시 경로에 한글이 있으면 오류가 난다. 이를 위해 다른 폴더로 설정 or 폴더명을 바꿔줘야 unicode 오류가 발생하지 않는다.

```python
import pandas as pd
import numpy as np
from konlpy.tag import Okt
from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 불용어(stopwords) 정의
stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']

# 데이터 로드 및 중복 제거, Null 값 제거
train_data = pd.read_table('ratings_train.txt')
test_data = pd.read_table('ratings_test.txt')

train_data.drop_duplicates(subset=['document'], inplace=True)
test_data.drop_duplicates(subset=['document'], inplace=True)

train_data['document'] = train_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
test_data['document'] = test_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")

train_data = train_data.dropna(how='any')
test_data = test_data.dropna(how='any')

# 형태소 분석기(Konlpy)를 사용하여 형태소 단위로 문장을 토큰화
okt = Okt()

X_train = []
for sentence in tqdm(train_data['document']):
    tok_sen = okt.morphs(sentence, stem=True)
    stop_rm_tok = [word for word in tok_sen if not word in stopwords]
    X_train.append(stop_rm_tok)
    
# 토큰화된 텍스트 데이터를 정수 시퀀스로 변환
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)

# 시퀀스 데이터를 일정한 길이로 패딩
max_len = 30  # 적절한 시퀀스 길이 설정
X_train = pad_sequences(X_train, maxlen=max_len)
```

### 2. 막대 그래프로 레이블 분포 시각화

```python
import matplotlib.pyplot as plt

# 레이블(클래스) 분포 확인
label_count = train_data['label'].value_counts()
label_names = ['Negative', 'Positive']

# 막대 그래프로 시각화
plt.bar(label_names, label_count)
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.title('Distribution of Sentiment Labels')
plt.show()
```

### 3. LSTM 모델 구성

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# LSTM 모델 구성
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_len))
model.add(LSTM(128))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```