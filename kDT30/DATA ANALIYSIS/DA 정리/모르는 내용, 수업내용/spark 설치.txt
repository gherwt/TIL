- spark : 하둡의 맵리듀스라는 데이터 처리(저장, 추출, 변환 등) 기능을 프레임으로 구현한 오픈소스
- 하둡 설치 후 spark를 얹어서 사용한다.
- 필요 환경

1. 파이썬 
- which python3 로 파이썬의 위치를 확인할 수 있다.
 
2. 주피터 노트북 
- pyspark를 사용하게 됨 

3. java 
- 하둡스파크가 자바 베이스이기 때문

4. hadoop

5. 스파크

6. pyspark


- 스파크 설치

### 하둡이 없는 스파크 설치
- #ref : https://www.apache.org/dyn/closer.lua/spark
- 다운 버전은 확인하고 다운 받아야 한다.

### 이전 버전
wget https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-without-hadoop.tgz 

## 환경 변수 설정

- spark 어느 위치에서나 실행가능하도록 설정해준다.

- cd

- vim ~/.bashrc

export SPARK_HOME=/root/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

esc :wq

- 설정 환경 변수 시스템에 적용
source ~/.bashrc


### 스파크 설정
- spark와 hadoop을 연결 위한 설정
- spark 설정파일 : spark -env.sh
- 템플릿 저공하므로 복사해서 사용한다
- cd $SPARK_HOME/conf
- cd spark/conf
- cp spark-env.sh.template spark-env.sh

- vim spark-env.sh

# spark 실행할 때 hadoop 위치 지정 - spark 소스코드가 hadoop base로 build(실행상태) 됨
export SPARK_DIST_CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath)
# spark load data 저장소를 hadoop으로 지정
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

- pyspark 설치

# --no--cache-dir : cache 사용하지 말것(ram 부족한 경우)
- pip install pyspark==3.2.4 --no--cache-dir

- pyspark을 사용하기 위한 환경변수 설정(파이썬 위치를 등록한다.)
- 파이썬3 위치 확인
- 파이썬3 위치를 spark-env.sh에 등록한다.

cd
which python3

cd spark/conf
vim spark-env.sh
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3
esc :wq

- spark 실행 확인
- spark 실행 전 hadoop 실행해야 함
start-dfs.sh
start-yarn.sh

pyspark --master yarn

- pyspark 의 기본 리소스 매니저 yarn 으로 설정
- spark-default.con 파일에서 진행
cd

cp spark-defaults.conf.template spark-defaults.conf
vim spark-defaults.conf
spark.master		yarn
esc :wq