### CNN(컨볼루션 신경망)

#### `Conv2D()`

- 케라스에서 컨볼루션 층을 추가하는 함수

```py
# 커널 수 : 32
# 커널 크기 : kernel_size=(3, 3) (3x3 or 5x5 형태)
# 입력값 : input_shape=(28, 28, 1) (행, 열, 색상 또는 흑백) 
# relu 를 활성화 함수로 사용한다.
model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))
```

```py
# 모델 설정 
# 컨볼루션 레이어 설정하기
model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28, 28, 1)))

# max pooling -> 다운 샘플링을 통해 과적합을 줄이고 계산 비용을 감소시킨다.
model.add(layers.MaxPooling2D((2,2,)))
```

```py
# 모델에 합성곱층을 전체적으로 보기
model = tf.keras.models.Sequential([
    tf.keras.layers.Rescaling(1/255, input_shape = (300, 300, 3)),
    #input층 생성시 이미지 전처리를 진행하게 설정
    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
```

#### `tf.keras.layers.Flatten()`

- 특성맵을 1차원으로 변경해준다.

- 이미지 데이터를 완전 연결 레이어에 전달하기 위해서 1차원 벡터로 바꿔줘야 한다.

- 가중치를 가지지 않으며 단순히 평평하게 만들어주는 역할만을 수행한다.

```py
    tf.keras.layers.Flatten()
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

#### EarlyStopping

- 모델 최적화 단계에서 학습 자동 중단 설정
- 과적합이 되지 않도록 모니터링하면서 학습을 중단시키는데 사용한다.

```py
# ModelCheckpoint: 이 콜백은 모델이 학습 중에 검증 데이터의 손실 함수를 모니터링하고, 검증 손실이 최소화되었을 때 모델을 파일로 저장하는 역할을 한다.
checkpointer = ModelCheckpoint(filepath = modelpath,
                              monitor = 'val_loss',
                              verbose = 1,
                              save_best_only = True)

# 검증 데이터의 손실 함수를 모니터링하고, 지정된 횟수(patience)만큼 연속으로 검증 손실이 향상되지 않을 경우 학습을 자동으로 중단시킴.
# patience - 연속으로 검증 손실이 향상되지 않아도 학습을 몇 번 더 진행할지 지정.
early_stopping_clbk = EarlyStopping(monitor = 'val_loss', patience = 10)
```

#### dropout

- `과적합을 방지`하고 모델의 일반화 성능을 향상시키기 위한 정규화 기법 중 하나이다.

1. **학습 단골에서 무작위로 일부 뉴런을 비활성화:** Dropout은 학습 데이터를 통해 모델을 학습할 때, 각 학습 단계(epoch)에서 무작위로 일부 뉴런을 비활성화시킨다. 이때, "일부 뉴런"은 뉴런의 일부 출력을 랜덤하게 0으로 만드는 것을 의미.

2. **모델 앙상블 효과:** 이 과정은 각 학습 단계에서 다른 뉴런 그룹을 비활성화하므로, 이는 여러 개의 서로 다른 모델을 학습하는 것과 유사한 효과를 냅니다. 이렇게 여러 모델의 평균 예측을 내는 것이 모델의 일반화 능력을 향상시킬 수 있습니다.

3. **과적합 방지:** Dropout은 모델이 학습 데이터에 지나치게 적합되는 것을 방지합니다. 뉴런들이 무작위로 비활성화되기 때문에 모델이 특정 훈련 예제에 지나치게 의존하지 않도록 합니다.

4. **하이퍼파라미터로 조절:** Dropout의 정도는 하이퍼파라미터로 조절할 수 있습니다. 일반적으로 `0.2 ~ 0.5`정도의 드롭아웃 비율이 사용됩니다.

Dropout은 주로 인공 신경망의 은닉층에 적용, 케라스(Keras)와 같은 딥러닝 프레임워크에서 쉽게 적용 가능하다.