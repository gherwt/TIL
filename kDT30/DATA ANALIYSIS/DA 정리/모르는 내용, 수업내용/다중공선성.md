
내일 마저 정리https://ysyblog.tistory.com/122

https://dacon.io/codeshare/4443

# 다중공선성이란?

- 독립변수(X) 는 종속변수(Y)하고만 상관 관계가 있어야 하며, 독립 변수들끼리 상관 관계가 있어서는 안된다.

- 독립 변수 간 상관관계를 보이는 것을 다중공선성(Multicollinearity)이라고 한다.

- 다중공선성이 발생시, 부정확한 회귀 결과가 도출될 수 있다.
  
  - 계수 추정이 잘 되지 않거나, 불안정해져서 데이터가 약간만 바뀌어도 `추정치가 크게 달라짐`
  
  - 계수가 통계적으로 유의미하지 않은 것처럼 도출
  
  - 모형 `과적합`의 위험성

## 다중 공선성 확인방법

### 1. 산점도 그래프

- 산점도 그래프

```py
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
from statsmodels.datasets.longley import load_pandas
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor
plt.rc('font', family='NanumBarunGothic') ## 한글 폰트

dfy = load_pandas().endog
dfX = load_pandas().exog
df = pd.concat([dfy, dfX], axis=1)
```

- 상관관계 매트릭스

```py
fig, ax = plt.subplots(figsize=(11, 11))
df_corr = dfX.corr()

# mask
mask = np.triu(np.ones_like(df_corr, dtype=np.bool))

# adjust mask and df
mask = mask[1:, :-1]
corr = df_corr.iloc[1:,:-1].copy()

# color map
cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)

# plot heatmap
sns.heatmap(corr, mask=mask, annot=True, fmt=".2f", 
            linewidths=5, cmap=cmap, vmin=-1, vmax=1, 
            cbar_kws={"shrink": .8}, square=True)

# ticks
yticks = [i.upper() for i in corr.index]
xticks = [i.upper() for i in corr.columns]
plt.yticks(plt.yticks()[0], labels=yticks, rotation=0)
plt.xticks(plt.xticks()[0], labels=xticks)

# title
title = 'CORRELATION MATRIX\n'
plt.title(title, loc='left', fontsize=15)
plt.show()
```

### 2. VIF(Variance Inflation Factors, 분산팽창요인)

- 일반적으로 5 혹은 10보다 크면 다중공선성이 있다고 판단

```py
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(dfX.values, i) for i in range(dfX.shape[1])]
vif["features"] = dfX.columns
vif

```

## 다중 공선성 해결 방법

### 1. 정규화 사용

```py
model2 = sm.OLS.from_formula("TOTEMP ~ scale(GNP) + scale(ARMED) + scale(UNEMP)", data=df_train)
result2 = model2.fit()

print(result2.summary())
```

### 2. 의존적인 변수를 삭제

- 변수 제거하고 모델링

```py
test2 = []
for i in range(10):
    df_train, df_test = train_test_split(df, test_size=0.4, random_state=i)
    model2 = sm.OLS.from_formula("TOTEMP ~ GNPDEFL + POP + GNP + YEAR + ARMED + UNEMP", data=df_train)
    result2 = model1.fit()
    test2.append(calc_r2(df_test, result2))

test2
```

### 3. PCA(principal component analysis) 방법으로 의존적인 성분을 삭제한다.