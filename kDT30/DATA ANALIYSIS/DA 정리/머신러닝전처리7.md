### 이상치 데이터

- 데이터 특성의 분포를 볼 때 주로 나타나는 값의 분포가 아니고 값이 너무 크거나 너무 작은 경우 `이상치`로 판단한다.

- 보통은 `사분위수를 기준으로 이상치를 판단`하지만 도메인적인 판단이 가능함

- 사분위수로는 이상치지만 도메인적인 판단으로 정상데이터로 처리한다면 왜도가 심한 데이터의 분포로 나타날 수 있음

- 왜도 처리 전처리를 따로 진행해야한다.
  
#### 사분위수

- 사분위수 : 데이터 집단을 크기 순으로 나열했을 때 4개(25%로 나눠준다.)의 구간으로 나눌 수 있고, 구간의 경계점을 1, 2, 3, 4분위수라고 명칭함

    - Q1, Q2, Q3 로 표현되어 있다.
    
    - IQR : 데이터의 `분포가 가장 많은 구간`, 지점 Q3 ~ Q1 구간
    
    - Outlier : 이상치
        
        - Upper : Q3 보다 값이 큰, Maximum, Q3 + (IQR * 1.5)
        
        - Lower : Q1 보다 값이 작은, Minimum, Q1 - (IQR * 1.5)
        
        - 이상치는 학습데이터를 기준으로 설정

![이상치 판단 박스 그래프 사진](attachment:5b497158-9d28-4bd7-b579-bb1c8ff4f814.png)

#### IQR_rule을 계산하는 함수를 생성해서 사용

```py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv("./데이터/glass.csv")

# Tpye 을 종속변수로 설정
X = df.drop('Type', axis = 1)
y = df['Type'] 

# 훈련/평가 데이터로 분리해서 전처리를 진행
Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, random_state = 4)


# val_list로는 컬럼값을 전달(열벡터)
def IQR_rule(val_list):
    
    # IQR 계산
    # Q1 1분위수, Q3 3분위수
    Q1 = np.quantile(val_list, 0.25)
    Q3 = np.quantile(val_list, 0.75)
    
    # 1~3 분위수의 사이값을 구한다.
    IQR = Q3 - Q1

    # IQR rule의 위배 여부를 표현하는 bool list 를 계산해서 반환
    # 이상치의 조건 :  Q3 + 1.5 * IQR 초과하거나 Q1 - 1.5 * IQR 미만이거나
    # Q1 ~ Q3 사이값을 구하는 조건
    # val_list가 상위 이상치 이하 = True
    # val_list가 하위 이상치 이상 = True
    not_out_cond =  (Q3 + 1.5 * IQR > val_list) & (Q1 - 1.5 * IQR < val_list)
    
    # 이상치가 없는 값
    # Q1, Q3, IQR 값은 저장해야 함
    return not_out_cond

# RI 에 이상치 판단을 해준다.
tmp = IQR_rule(df['RI'])
```

#### `df.apply(함수명)`
  
  - 판다스(또는 다른 데이터 분석 라이브러리)에서 `데이터프레임` 또는 `시리즈의 행 또는 열에 함수를 적용하는데 사용`되는 강력한 메서드
  
  -  apply 함수를 사용하면 데이터프레임의 각 행 또는 열에 대해 사용자가 지정한 함수를 적용할 수 있습니다.
  
  - 벡터화 연산을 가능하게 하는 함수이다.

```py
# axis = 0은 열마다 함수를 적용하고, axis = 1은 행마다 함수를 적용
# Train 에 IQR 함수를 적용해준다.
conditions = Train_X.apply(IQR_rule)
```

#### 이상치 제거시 유의 사항

- 이상치 확인은 `특성별로(column)` 진행, 삭제는 `레코드(row)`별로 진행

- 레코드의 `각 특성 중 이상치가 하나라도 있으면 해당레코드는 제거를 하는게 일반적인 방법`

- 이상치를 제거했을 때 학습데이터 수가 급격히 줄어들면, 이상치 범위를 좁혀서 데이터를 유지하는 방법을 먼저 사용해본다. 

```py  
# 학습 Data의 컬럼의 수를 확인한다.
len(Train_X.columns)

# condition 의 각 레코드 값을 합산했을 때 8과 같으면 이상치가 없는 레코드이므로 사용한다.

# 8이 아니라면 사용하지 않는다.
# 열(특성)별로 진행
tot_cond = conditions.sum(axis = 1) == (len(Train_X.columns))

# Train에서 'tot_cond' 를 선택하여 저장해준다.
Train_X = Train_X.loc[tot_cond]
```

### 이상치 판단 방법

#### 밀도 기반 군집화 수행(Density-Based Clustering, 밀도 기반 군집화)

- DBSCAN등의 밀도 기반 군집화 기법은 군`집에 속하지 않은 샘플을 이상치라고 간주`하므로, 밀도 기반 군집화 결과를 활용하여 이상치를 판단할 수 있음

- DBSCAN은 `중심점과 경계점이 아닌 모든 샘플을 이상치`라고 판단함.

- 단, 이상치 제거 성능은 좋다고 알려져 있으나 DBSCAN등의 밀도 기반 군집화 모델의 파라미터 튜닝이 쉽지 않다는 단점이 있음
![image-2.png](attachment:image-2.png)

#### sklearn.cluster.DBSCAN

- DBSCAN를 수행하는 인스턴스를 생성하는 함수

- 주요 파라미터

    - eps(epsilon 값(`반경`)) : 이웃이라 판단하는 반경(원의 반지름)
    
    - min_samles : "MinPts"을 설정, 중심점이라 판단하기위해, eps 내에 들어와야하는 최소 샘플 수, 중심점 자신도 포함한다. `최소 이웃 데이터 포인트 수`
    
    - metric : 사용하는 `거리 척도`
 
- 주요 속성
    - .labels_ : 각 샘플이 속한 군집정보(-1: noise(이상치))

```py
import pandas as pd
import numpy as np
from scipy.spatial.distance import cdist
from sklearn.cluster import DBSCAN
from sklearn.model_selection import train_test_split
df = pd.read_csv("./데이터/glass.csv")

# Type 을 종속변수로 설정하고 훈련/평가 데이터로 분리
X = df.drop('Type', axis = 1)
y = df['Type'] 
Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, y, random_state = 4)

# DBSCAN 의 파라미터를 설정하기 위해 생성
# 군집 및 거리를 파악하기 위해 Train_X 로 2차원으로 거리행렬 생성
# cdist 함수를 사용하여 Train_X 데이터의 거리 행렬(Distance Matrix, DM)을 계산
DM = cdist(Train_X, Train_X) 

# 2차원 거리 행렬 DM을 1차원 배열인 DM_t로 변환
DM_t = np.ravel(DM)
# 0.6326153202063973

# 1차원 거리 데이터 DM_t에서 10% 에 해당하는 값을 계산, 이를 이상치를 판단하는 임계값으로 활용
np.quantile(DM_t, 0.1)
# 0.6326153202063973

# 2차원 거리 행렬 DM에서 10% 에 해당하는 값을 계산, 이 값도 이상치 판단에 활용
np.quantile(DM, 0.1)
```

- 특정 컬럼에 대한 이상치 판단이 아니다. 전체 레코드의 `특성상 어느 군집에도 포함되지 않으면 이상치로 판단`하겠다.

```py
# Train_X 데이터에 대한 DBSCAN 군집화를 수행한다.
# eps 이웃을 정의하는 반경 0.63 은 cdist 로 구한 임계값 이정도 거리 안에 있으면 군집이라고 판단하겠다.
# min_samples는 핵심 포인트가 되기 위한 이웃 데이터 포인트의 최소 개수 4개가 주변에 있으면 군집의 중심 포인트라고 판단.
cluster_model = DBSCAN(eps = 0.63, min_samples = 4).fit(Train_X)

# 배열에서 -1에 해당하는 값의 개수를 세어 이상치로 판단된 데이터 포인트의 수를 확인, False 값을 확인한다.
print(sum(cluster_model.labels_ == -1))
# 42개가 이상치로 판단 => 전체 레코드 160개 중  1/4 정도
# 다시 한 번 전처리 과정이 필요하다고 판단

# 위 과정을 반복 DBSCAN 결과에서 군집 번호가 -1인 데이터 포인트를 제외한 나머지 데이터를 Train_X로 다시 할당합니다.
cluster_model = DBSCAN(eps = 1.3, min_samples = 4).fit(Train_X)

print(sum(cluster_model.labels_ == -1))
# 23개면 적당하다고 판단, 삭제를 수행

# 군집번호가 -1 이 아닌 레코드만 다시 Train_X 로 할당한다.
Train_X = Train_X[cluster_model.labels_ != -1]

Train_X.shape
```