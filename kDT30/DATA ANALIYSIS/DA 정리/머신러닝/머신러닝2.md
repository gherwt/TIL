## 생선 분류 문제

- 고전적인 프로그램 중 하나이다.

- 생선의 길이와 무게에 따라서 생선을 분류한다.

```py
# 주어진 생선에 대한 길이 정보
fish_lengths = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7,
                31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5,
                34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0,
                38.5, 38.5, 39.5, 41.0, 41.0]

# 도미의 길이는 30cm가 넘는다 라는 정보가 있어서 도미 구분 기준을 30cm로 설정.
for fish_length in fish_lengths:
    if fish_length > 30:
        print('도미')
    else:
        print('도미 아님')
```

- 하지만 위의 자료만으로는 도미에 대한 정확한 기준을 얻기 힘들다.

- 도미에 대한 패턴에 대한 학습을 시키자. -> 머신러닝, 알고리즘을 활용하자.

- 하지만 30cm보다 크면 무조건 도미일까?

- 고래와 새우처럼 아주 큰 절대적이 차이가 있지 않는 한 절대기준을 정하기는 어렵다. 때문에 머신러닝(주어진 데이터에서 패턴을 학습해서 스스로 기준을 찾는 기법)을 활용해서 학습 후 도미를 구분하는 알고리즘을 생성한다.

### 도미 vs 빙어 분류하기

- 2개의 `클래스`(분류값, 도미/빙어 두개 중 하나로 분류)

- 이진 분류(도미, 도미x = 빙어, 분류가 2개인 경우)

- 길이 제외 무게 변수도 추가해준다.

```py
# 특성이 많을수록 다양한 패턴을 통해 기준을 상세히 설정할 수 있다. -> 경우의 수가 많아지기 때문이다.
# 기준 복잡 -> 세세한 기준 -> 좋은 결과를 도출 가능하다.

# 도미 길이
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7,
                31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5,
                34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0,
                38.5, 38.5, 39.5, 41.0, 41.0] 

# 도미 무게
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0,
                500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0,
                610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0,
                714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
```

- 이 때 도미의 길이, 무게는 도미의 특성을 나타내는 특성(feature)이다.

- 특성은 데이터를 표현하는 하나의 성질이다. 이 두 가지 특성 간의 관계를 쉽게 파악하기 위해 산점도로 표현해 본다.

```py
import matplotlib.pyplot as plt
plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 선형적인 관계를 보여준다. 즉 길이가 증가하면 무게가 증가한다.
# 길이가 x(독립변수), 무게가 y(종속변수) 이기 때문이다. 
```

![Alt text](%EB%8F%84%EB%AF%B81.png)

### 빙어 데이터

- 도미와 비교하기 위해서 빙어 데이터도 가져와준다.

```py
# 빙어 길이
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2,
                12.4, 13.0, 14.3, 15.0]

# 빙어 무게
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2,
                13.4, 12.2, 19.7, 19.9]
```

- 마찬가지로 산점도로 표현하기

```py
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 자료가 도미보다 적지만 어느정도 도미와 같은 선형관계가 나타난다고 할 수 있다.
```

![Alt text](%EB%B9%99%EC%96%B41.png)

### 도미 데이터와 빙어 데이터 병합한 산점도

- 두개의 생선에 대한 정보를 한 그래프로 표현하면 두 생선의 차이 등을 한번에 확인해보자.

```py
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 도미는 길이가 길어나면 무게가 증가한다.
# 빙어는 길이가 증가해도 무게 증가가 미비하다.
```

![Alt text](%EB%8F%84%EB%AF%B8%EB%B9%99%EC%96%B4.png)

### 머신러닝 프로그램

- 머신러닝을 가능하게 하는 알고리즘은 많은 시간을 거쳐서 완성되어 왔고, 또 많은 이론들이 발표되고 있다.

- 그 중에서 몇 알고리즘들은 검증되어져서 실제 사용할 수 있는 알고리즘들이며 머신러닝을 배운다는 것은 발표되어 알려지고 여러 연구 및 기업에서 실제 사용해 검증된 알고리즘들의 개념을 배우고 활용법을 배우는 것이 목적이다.

- 머신러닝 및 딥러닝의 결과물을 모델링이라고 하는데 이 모델링은 여러 모델(알려진 알고리즘)을 하나의 데이터로 셋팅해서 테스트 하고 성능을 평가한 후 가장 성능이 좋은 모델을 찾아내는 과정이다.

- 머신러닝 및 딥러닝은 100%의 예측을 목표로 하는 것이 아니고 가장 성능이 좋은 `예측율이 높은 모델링을 찾아내는 것`이 목표로 하고 있다.

- 머신러닝, 딥러닝은 `오류를 인정하고 오류를 최소화 하는것` 이 목표이다.

### Scikit-learn 패키지를 사용한 도미, 빙어 데이터 분류하기

#### Scikit-learn

- 파이썬 프로그래밍 언어용 자유 소프트웨어 기계 학습 라이브러리임

- 다양한 분류, 회귀, 그리고 서포트 벡터 머신, 랜덤 포레스트, 그라디언트 부스팅, k-평균, DBSCAN을 포함한 클러스터링 알고리즘을 라이브러리로 갖고 있다.
- 파이썬의 수치 및 과학 라이브러리 NumPy 및 SciPy와 함께 운용되도록 설계되었다.

#### 두 생선 데이터 병합하여 데이터 준비

- 머신러닝은 분류하고자 하는 두 생선의 여러 샘플(도미와 빙어의 몸무게, 길이)을 알고리즘(`클래스로 구성`되어 있음)에 주입하여 해당 샘플들을 학습(`특성을 찾는것`)시켜 기준을 만들어내는 것임, 하나가 아닌 2개의 생선 데이터이기 때문

- 학습에 사용되는 계산식이 알고리즘이고, k-최근접 이웃 알고리즘을 사용해서 두 데이터를 분류하겠다.

```py
# 두 리스트 병합하기
# 길이 따로 무게 따로 현재 존재

length = bream_length + smelt_length
weight = bream_weight + smelt_weight

# 생선 한마리당 길이,몸무게로 정보를 정리, 묶어서 한꺼번에 볼 수 있도록 하겠다.
# 두 개 이상의 자료에서 동일한 위치에 있는 요소들을 튜플로 묶어주는 함수인 zip 을 이용해 자료를 묶어준다.
# 이는 현재 사용하려는 sk-learn 패키지가 원하는 형태로 데이터 구조를 변환하기 위함이다.
# 현재 튜플 형태임- > 2차원 리스트로 변환

test = zip(length, weight)
for i in test:
    print(i)


# 독립변수인 길이, 무게 정보 // 특성 2가지를 수집
# l, w 으로 특성을 분류해준다.
fish_data = [[1, w]for i, w in zip(length, weight)]
print(fish_data)
```

### 타겟(레이블) 데이터 생성

- 데이터를 입력하여 예측을 하는 `지도학습`이므로 각 생선이 도미인지 빙어인지 구분하는 값이 있어야 학습을 진행 할 수 있음

- 찾으려는 대상을 1로 놓고 나머지 대상은 0으로 놓는다(안정적으로 진행하기 위함이다.)

- 예제는 도미를 찾는것이 목표이므로 도미를 1로 빙어를 0으로 설정

- 단, 이진(두개 중 하나를 찾는 것)분류 이므로 0으로 예측하면 빙어로 구분한다.

```py
# 35 마리의 도미, 14마리의 빙어가 있다. 이를 타겟 으로 지정해준다.
fish_target = [1] * 35 + [0] * 14
print(fish_target)
```

### K-최근접 이웃(K-Nearest Neighbor) 알고리즘을 이용한 분류

### 사이킷런의 K-최근접이웃 클래스 진행 과정

1. 클래스 객체 변수 생성

    - 하이퍼파라미터(파라미터와 구분하여 사용자가 딥러닝을 위해 설정하는 값들을 모두 지칭) 설정해준다.

2. 객체변수의 메서드(함수) `fit()` 이용해서 `학습을 진행`한다.
    - 학습을 위한 `종속변수`(타겟변수)와 `독립변수` 전달

    - 학습이 진행된 클래스 객체 변수 - `모델`

3. 테스트 진행
    - 객체변수의 메서드`score() 함수를 이용해서 테스트 진행`
    - 테스트 데이터를 전달

#### 클래스 객체 변수 생성

```py
from sklearn.neighbors import KNeighborsClassifier

# KNeighborsClassifier 활용해 k 최근접이웃 클래스 객체를 생성한다.
# kn 이라는 객체변수 생성
# KNeighborsClassifier 클레스의 모든 속성과 메서드를 사용할 수 있는 변수이다.

kn = KNeighborsClassifier()
```

#### 훈련(training) : `fit(features, label) 메서드`

```py
# 도미를 찾기 위한 기준을 학습시킴
kn.fit(fish_data, fish_target)
```

#### 모델 평가: `score()` 메서드

```py
# 정확도를 나타내줌
kn.score(fish_data, fish_target)
```

- 여기서 정확도란?

- 정확한 답을 몇개 맞추었는지를 백분율로 나타낸 값을 의미한다.

- 정확히 맞힌 개수 / 전체 데이터 수

- 위 데이터는 학습시킬 때 완벽히 분류된 데이터를 입력했기 때문에 정확도가 100이 나오지만 이는 불가능한 값이고 좋지 않은 값이다.

### K-최근접 이웃 알고리즘을 이용한 추가된 새로운 데이터 분류

- 새로 생선 하나가 들어와서 이 생선이 도미인지 빙어인지를 알려달라고 함

- 길이(30), 무게(600)에 대한 정보가 전달됨

```py
# 새로운 생선 30,600을 그래프에 표현해 보면
import matplotlib.pyplot as plt
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)

plt.scatter(30, 600, marker = '^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
# 도미의 값 사이에 있기 때문에 데이터가 존재 도미로 판단할 수 있다.
```

![Alt text](new%EC%83%9D%EC%84%A0.png)

#### predict 함수 사용하기

- 새로운 데이터로 어떤 생성인지 판단하기 위해서는 모델객체의 predict() 함수를 사용한다.

- 새로운 데이터는 모델이 학습한 데이터와 같은 형태로 전달되어야 함

- 사이킷런의 모델 알고리즘은 2차원 데이터를 요구한다. 전달받은 데이터를 2차원 데이터로 변환하는 작업을 해준다.

```py

kn.predict([[30, 600], [50, 550], [5, 20]])

# ._fit_X  속성: 학습에 사용한 data(fish_data)를 가지고 있음(저장하고 있는 속성)
print(kn._fit_X)

# ._y 속성 : 학습에 사용한 결정값(class값, fish_target) 을 가지고 있음
print(kn._y)
```

- k-최근접 이웃 알고리즘은 학습 데이터를 모두 저장하고 있다가 새로운 데이터가 들어오면 새로운 데이터로부터 가까운 직선거리에 어떤 데이터가 있는지 확인만 하면 된다.-> 가장 간단한 알고리즘이다.

- 단점

```md
    - 데이터가 아주 많은 경우라면??? -> 데이터를 보관하기 위해 아주 많은 메모리가 필요, 가까운 거리에 있는 데이터를 찾기 위해서 많은 계산이 필요.

    - 새로들어온 데이터로부터 기존에 저장한 데이터(학습데이터)들 까지의 거리를 계산하는데 시간이 많이 걸린다.
```

### KNN의 중요한 하이퍼 파라미터: K-neighbors

- 몇개의 가까운 데이터를 참고하는지의 기준

- `n_neighbors =` 를 통해 크기를 지정해준다.

```py
# 49개의 데이터를 참고하자
kn49 = KNeighborsClassifier(n_neighbors = 49)

# k = 49경우 모델 훈련 및 성능
# 모델 학습
kn49.fit(fish_data, fish_target)

# 정확도 측정
kn49.score(fish_data, fish_target)
# 71% 의 정확도가 나왔다.
# 49의 data 에서 다수결로 결정 도미 > 빙어
```

- 하지만 **49개 중 도미가 35개 이므로 어떤 데이터를 넣어도 무조건 도미로 예측**

- **why? 도미의 수가 더 많으므로**

- 그렇다면 n-neighbors의 수를 높게 설정하는 것이 모델의 성능을 높이는 방법인가???

- 49개로 잡았더니 성능이 떨어짐 그럼 최적의 n-neighbors 수는 몇마리인가?

- 이런 값들을 찾는것이 모델링이며, 최적의 n-neighbors수가 얼마인지 정답은 없다. `평가지표(정확도)가 높게 나오는 어떤 숫자를 찾아내는것이 목표`이다.

- n-neighbors 파라미터 처럼 모델링을 하는 사람이 설정할 수 있도록 하는 파라미터를 **하이퍼 파라미터** 라고 한다.
