## 특성 공학과 규제

- 농어의 무게를 예측한 결과, 학습 점수보다 테스트 점수가 더 높음(과소적합)

- 이 문제를 해결하기 위해 더 고차항을 넣어야 할것 같은데 얼만큼 고차항을 넣어야 할지도 모르고 수동으로 고차항을 넣기도 힘듬

- 이에 또다른 해결책 농어의 특성 데이터를 더 준비한다

- 선형회귀는 특성이 어느정도 많을수록 효과가 좋아진다, 더 좋은 성능을 나타난다.

### 다중 회귀

- 여러개의 `특성(독립 변수가 여러개)`을 사용한 선형 회귀
- 한개의 특성을 사용했을 때 선형회귀 모델을 직선을 학습한다면, 두 개의 특성을 사용하면 좌표가 증가 -> 즉 축이 증가, 특성이 두개면 타깃값과 함께 3차원 공간을 형성함
- 타깃 = a X 특성1 + b X 특성2 + 절편은 평면이 됨
- 그렇다면 특성이 3개인 경우는??
- 그림으로 표현할 수는 없지만 축이 하나 더 들어난다고 보면 됨
- 특성이 많은 고차원에서는 선형 회귀가 매우 복잡한 공간을 표현할 수 있다.
- 이를 활용해서 농어의 길이 X 농어의 높이 등 새로운 특성도 만들어서 회귀식에 추가해 보자

- 기존의 특성을 사용해 새로운 특성을 만들어 내는 것 : **특성공학**

- 사인킷런의 `PolynomialFeatures`를 사용하면 간단히 추가가 된다(특성공학을 간단히 진행가능)

### 특성이 여러 개인 경우​ -> 기울기가 2개 이상

- **다중`선형`회귀**
​
- $ y = w_0 + w_1 X_{1} + w_2 X_{2} + w_3 X_{3} + ... + w_n X_{n} $
- 독립변수(festure)와 종속변수(target)의 관계가 일차방정식으로 표현된 회귀
- 직선으로 표현, 2차원 이상, 축이 여러개이기 때문

- **다항 회귀  (Polynomial Regression)** (2차 방정식 이상)

- $ y = w_0 + w_1 X_{1} + w_2 X_{2} + w_3 X_{1}X_{2} + w_4 X_{1}^2 + w_5 X_{2}^2 $

- 회귀가 독립변수의 일차식이 아닌 2차식, 3차식 등 다항식으로 표현되는 회귀식
- 차수를 높히는 것 (2차 이상을 포함)

- $ z=[X_{1} , X_{2} , X_{1}X_{2} , X_{1}^2, X_{2}^2 ] $라고 하면

- $ y = w_0 + w_1 Z_{1} + w_2 Z_{2} + w_3 Z_{3} + w_4 Z_{4} + w_5 Z_{5} $

- 피처 X에 대해 타겟 Y 값의 관계를 단순 선형 회귀 직선형으로 표현한 것보다
- 다항 회귀 곡선형으로 표현한 것이 더 예측 성능이 높음
- **다항 회귀는 특성이 2개 이상이므로 다중회귀의 맥락과 같다**

### 본격적으로 다중 회귀를 실시

- 데이터 준비 및 가공

```py
import pandas as pd

# 특성을 추가 수집한 것 넓이, 높이, 두께
df = pd.read_csv('https://bit.ly/perch_csv')
perch_full = df.to_numpy()


import numpy as np
from sklearn.model_selection import train_test_split

# 타깃데이터
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
     1000.0, 1000.0]
     )

# 훈련, 학습 데이터로 분리하기
train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state = 42)
```

#### 다중 회귀 모델 훈련

- 특성이 2개 이상으로 늘어났으므로 평면을 학습하는 다중회귀로 진행한다.

```py
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(train_input, train_target)

print(lr.score(train_input, train_target))
print(lr.score(test_input, test_target))

# 회귀계수와 절편확인
print(lr.coef_, lr.intercept_)
# [9.07538153 69.46401443 38.00385678] -599.1708082981097
```

- 한 개의 특성으로 고차항을 생성했을 때보다는 학습/평가 점수가 낮게 나온다

- 단 특성이 하나일 때보다는 성능이 좋아졌다.

### 사이킷런의 변환기

- **사이킷런은 다항회귀를 위한 클래스를 명시적으로 제공하지 않는다.**

- 다항 회귀가 선형 회귀이므로 PolynomalFeatures 클래스를 통해 독립변수(피처)를 다항식 피처로 변환, 대신 다항 회귀 역시 선형 회귀이기 때문에   비선형 함수를 선형 모델에 적용시키는 방법을 사용해 구현  

- `PolynomialFeatures(degree=차수, include_bias=True/False)`

```py
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# 입력 데이터 생성 (2차원 배열)
X = np.array([2, 3])
X = X.reshape(-1, 1)

# 다항식 특성 생성 (2차 다항식)
# degree 매개변수를 사용하여 몇 차 다항식을 만들지 결정
# include_bias 매개변수를 통해 절편 항을 추가할지 여부를 결정
poly = PolynomialFeatures(degree = 2, include_bias=False)

# fit 메서드로 학습하고 transform 메서드로 변환 작업을 따로 수행
poly.fit(X)
poly_ftr = poly.transform(X)

print('변환된 2차 다항식 계수 :', poly_ftr)
#(1,X,X^2)

# 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성

X = np.arange(6).reshape(3, 2)
print('일차 단항식 계수 feature:\n', X)

# [[0 1]
#  [2 3]
#  [4 5]]

# PolynomialFeatures를 이용하여 degree = 2 인 2차 다항식으로 변환
poly = PolynomialFeatures(degree = 2)

# transform from (x1, x2) to (1, x1, x2, x1^2, x1*x2, x2^2)
# poly_ftr에는 X를 2차 다항식으로 변환한 결과가 저장
poly.fit(X)
poly_ftr = poly.transform(X)

# 변환된 결과는 (1, x1, x2, x1^2, x1*x2, x2^2)의 형태로 각각 상수항, x1, x2, x1의 제곱, x1과 x2의 곱, x2의 제곱을 나타내다.
print('변환된 2차 다항식 계수 feature:\n', poly_ftr)
```

### 예제 데이터를 다항식으로 변경하기

```py
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures()
poly.fit([[2, 3]])

poly = PolynomialFeatures(degree = 2, include_bias = False)
poly.fit(train_input) # 다항을 생성하기 위한 식을 생성
train_poly = poly.transform(train_input)

poly.get_feature_names_out()

# array(['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2','x2^2'], dtype=object)
```

### 다중 회귀 모델 훈련하기

- 다중 회귀, 단항 회귀 모두 선형회귀 모델로 훈련함

- 여러개의 특성을 사용하여 선형회귀를 수행

```py
from sklearn.linear_model import LinearRegression
lr.fit(train_poly, train_target)

print(lr.score(train_poly, train_target))
# 0.9903183436982126

# 테스트 세트의 점수 확인
print(lr.score(test_poly, test_target))
# 0.9714559911594125
```

#### 학습 데이터의 평가 지표가 0.02 정도 더 높다

- 과대적합이라고 이야기할 수도 있지만 아주 큰 차이는 아니기 때문에 타협할 수 있는 수치이다.
- `미묘한 차이라도 과소적합이 일어나면 모델 학습을 다시 고민`해야 함(학습이 모자라다는 의미이기 때문이다)

#### 특성을 더 많이 추가

- 3,4 제곱등의 특성을 추가해보자

```py
# 5제곱까지의 특성을 만들어보자
poly = PolynomialFeatures(degree = 5, include_bias = False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)

# 특성의 개수가 55개
print(train_poly.shape)
print(test_poly.shape)
```

- **샘플 갯수보다 특성이 더 많다**

```py
# 훈련 후 훈련데이터 스코어
# 거의 완벽하게 훈련을 했다.
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
```

- 특성을 늘리면 훈련데이터에 대해 거의 완벽하게 학습한다. 하지만 훈련세트에 `과대적합`됨 -> 일반화되지 않고 `매몰`되어 버린다.
- 즉, 훈련세트에는 완벽하지만 훈련세트에만 완벽하고 나머지 데이터(테스트세트)에는 성능이 형편없어진다
- **특히, 지금 예제처럼 샘플갯수보다 특성갯수가 더 많으면 훈련세트에 대해서는 완벽한 훈련을 하게된다.**
- 다중공선성 : 독립변수들끼리의 관계성이 높아서 생기는 문제
- 독립변수는 서로 `관련이 없어야 좋은 결과`를 얻을 수 있다.

### 규제

- 머신러닝 모델이 훈련세트에 대해 너무 과도하게 학습하지 못하도록 가중치를 설정하는 것이라고 할 수 있다.
- 레코드별 모든 특성을 보고 패턴을 찾지 않고 일부 특성은 조금 적게 보는 것. 즉, 모델이 훈련세트에 과대적합되지 않도록 만드는 것
- 훈련데이터에만 적합한 식을 만들었다. 훈련데이터에 대해서만 완벽하게 학습해버렸다.
- **선형회귀 모델의 경우 특성에 곱해지는 계수(`기울기`)의 크기를 작게 만드는 것**

#### 스케일

- 규제를 적용할 때 특성값의 크기가 차이가 많이 나면 공정하게 제어되지 않음

- 규제 적용 전 `정규화(표준 정규화)`를 하는것이 일반적임

```py
# 다항 특성들을 표준화
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
# 특성들의 평균과 표준편차를 계산하고 저장
ss.fit(train_poly)

# 훈련세트로 학습된 변환기를
train_scaled = ss.transform(train_poly)
 # 테스트 세트에도 적용
test_scaled = ss.transform(test_poly)
```

### 릿지 vs 라쏘

- 선형회귀 모델에 규제를 적용한 알고리즘, 계수의 크기를 줄인다

- 릿지 : 계수를 `제곱한 값을 기준`으로 규제를 적용
  - 보통 릿지를 조금 더 선호 함
  - 계수의 크기를 아주 작게 줄일수는 있지만 0으로 만들지는 않는다(특성을 덜 바라본다.)

- 라쏘 : 계수의 `절대값을 기준`으로 규제를 적용
  - 라쏘는 계수를 아예 0으로 만들수도 있음(특성을 아예 없애버릴 수도 있다.)

#### 릿지

```py
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
ridge.score(train_scaled, train_target)
# 0.9896101671037343

# Ridge 모델을 사용하여 테스트 데이터에 대한 예측 수행
test_pred = ridge.predict(test_scaled)
# 테스트 데이터의 실제 목표값과 예측값을 비교하여 모델의 성능 평가
test_score = ridge.score(test_scaled, test_target)

# 테스트 점수 출력
print(test_score)
# 0.9790693977615387
```

- 5차항을 이용한 다항 회귀임, 규제 적용 전보다 훈련점수가 약간 낮아짐

- 테스트 점수는 정상적으로 돌아온다.

### 규제의 양을 조절

- alpha 값 : 사전에 사이언티스트가 조절하는 값(`하이퍼 파라미터`)

  - alpha값이 크면 규제 강도가 세지므로 계수값을 더 줄이고 조금 더 과소적합 되도록 유도
  - alpha값이 작으면 계수를 줄이는 역할이 줄어들고 선형회귀 모델과 유사해 짐

- 적절한 alpha값을 찾는 방법
  - alpha값에 대한 `R2 값의 그래프`를 그려보는 것
  - 훈련세트와 테스트 세트의 점수가 `가장 가까운 지점이 최적의 alpha 값`이 됨

```py
# alpha값에 따른 train/test score 저장을 위한 list 생성
import matplotlib.pyplot as plt

train_score = []
test_score = []

# 정해진 시작값은 없다.
# 하지만 보통 0.01 부터 10배 씩 증가시키면서 alpha 값을 찾는다.
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list :
    # 릿지 모델 생성
    ridge = Ridge(alpha = alpha)
    ridge.fit(train_scaled, train_target)
    # R2 점수 저장
    train_score.append(ridge.score(train_scaled, train_target))
    test_score.append(ridge.score(test_scaled, test_target))

# 저장된 alpha_list 의 값을 활용해 그래프를 그린다.
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
```

![Alt text](%EB%A6%BF%EC%A7%80.png)

- log10한 -1 이 가장 적절 -> 두 그래프 사이의 거리가 가장 좁기 때문
- np.log10(0.1) == -1

```py
# 위에서 찾아낸 최적 alpha 값 0.1 을 활용해서 훈련, 학습을 실행한다.
ridge = Ridge(alpha = 0.1)
ridge.fit(train_scaled, train_target)

print(ridge.score(train_scaled, train_target))
# 0.9903815817570367

print(ridge.score(test_scaled, test_target))
# 0.9827976465386928
```

- 훈련세트와 테스트 세트 점수 모두 비슷하게 높고 과대적합과 과소적합 균형을 맞추고 있는 결과가 나타난다.

```py
# 다항 특성들의 이름을 가져옵니다.
feature = poly.get_feature_names_out()
r_coef = ridge.coef_

# 릿지 회귀 모델의 계수인 r_coef와 해당 특성들의 이름을 포함하는 데이터프레임 coef_df를 생성
coef_df = pd.DataFrame({'Feature': feature, 
                        'Coefficient': r_coef})
print(coef_df)
```

#### 라쏘

- 위와 같은 방식으로 진행

```py
from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
# 0.989789897208096

print(lasso.score(test_scaled, test_target))
# 0.9800593698421883

# alpha값에 따른 train/test score 저장을 위한 list 생성
train_score = []
test_score = []

# alpha 값을 찾는다.
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list :
    # 라쏘 모델 생성
    lasso = Lasso(alpha = alpha, max_iter = 10000)
    lasso.fit(train_scaled, train_target)
    # R2 점수 저장
    train_score.append(lasso.score(train_scaled, train_target))
    test_score.append(lasso.score(test_scaled, test_target))

# 저장된 alpha_list 의 값을 활용해 그래프를 그린다.
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.show()
```

![Alt text](%EB%9D%BC%EC%8F%98.png)

- log10한 1 이 가장 적절 -> np.log10(10) == 1

```py
# 위에서 찾아낸 최적 alpha 값 0.1 을 활용해서 훈련, 학습을 실행한다.
lasso = Lasso(alpha = 10)
lasso.fit(train_scaled, train_target)

print(lasso.score(train_scaled, train_target))
# 0.9888067471131867

print(lasso.score(test_scaled, test_target))
# 0.9824470598706695
```

### 라쏘모델의 특징

- 라쏘 모델은 회귀 계수를 `0으로 만드는 성향`이 있음

- 계수(coef_)가 0인 특성의 개수 확인하기

```py
print(np.sum(lasso.coef_ == 0))
# 40 개가 나온다.
# 55개의 특성 중 40개의 특성의 계수가 0으로 변환
# 15개의 특성만 활용해서 모델링 진행되었음 -> 라쏘는 모델에 유용한 특성을 골라내는 용도로도 사용 가능

# 다항 특성들의 이름을 가져온다.
feature = poly.get_feature_names_out()

# Lasso 회귀 모델의 계수인 l_coef와 해당 특성들의 이름을 포함하는 데이터프레임 coef_df를 생성
l_coef = lasso.coef_
coef_df = pd.DataFrame({'Feature': feature, 
                        'Coefficient': l_coef})
coef_df

# 값이 0이 아닌 것들을 찾아준다.
coef_df[lasso.coef_ != 0]
```