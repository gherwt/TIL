### 피처 스케일링과 정규화 

#### 피처 스케일링

- 서로 다른 변수의 값 범위를 일정한 범위 수준으로 맞추는 작업

#### 방식

- Z-scaling(표준화)
    - 평균이 0이고 분산이 1인 가우시안 정규분포로 변화나는 과정
    - 주로 sklearn.preprocessing의 StandardScaler 모듈을 사용하여 작업을 수행한다.
    - 회귀(선형회귀(연속값 예측 회귀), 로지스틱회귀(분류))알고리즘 및 SVM 과 같은 알고리즘에 사용된다.
    
- Min-Max scaling
    - 0 ~ 1 사이의 값으로 변환해준다. 최소값을 0으로 변환, 최대값을 1로 변환
    - sklearn.preprocessing 의 MinMaxScaler 모듈을 사용하여 작업을 수행한다.
    - 데이터의 상대적인 크기와 범위를 보존하는데 사용되며, 신경망과 같은 알고리즘에서 주로 사용한다.
    - x_new = (xi - min(x)) / (max(x) - min(x))

```py
### StandardScaler 사용예제
from sklearn.datasets import load_iris # iris 데이터 저장 모듈
import pandas as pd

# data 불러오기
iris = load_iris()

# iris data를 처리하기 위해서 df 를 생성해준다.
iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names)
```

```py
from sklearn.preprocessing import StandardScaler

# 객체를 생성
scaler = StandardScaler()

# fit 매서드를 사용하여 입력데이터인 iris_df 의 평균과 표준편차를 계산 통계량을 구해준다.
scaler.fit(iris_df) 

# 표준화된 데이터를 새로운 데이터프레임으로 만들어 출력
iris_sc_df = pd.DataFrame(data = scaler.transform(iris_df), columns = iris.feature_names)

# 둘의 평균값을 비교해준다.
print('변환 전 특성의 평균값 :', iris_df.mean())
print('변환 후 특성의 평균값 :', iris_sc_df.mean())
```

### Min - MaxScaler

- 데이터 정규화: Min-Max 스케일링을 통해 데이터를 `0과 1 사이`로 변환하면, 각 특성의 중요도를 동등하게 취급합니다.

- 데이터를 정규화하고, 스케일링된 데이터를 사용하여 다양한 머신 러닝 알고리즘을 훈련할 때 도움이 됩니다.

- 변수 범위의 보존: Min-Max 스케일링은 변수의 범위를 보존하는 데 유용합니다.

- 신경망 및 SVM: Min-Max 스케일링은 신경망과 SVM과 같은 알고리즘에서 자주 사용됩니다. 

- 이러한 알고리즘은 입력 데이터가 일반적으로 0에서 1 또는 -1에서 1 범위 내에 있을 때 최적으로 작동한다.

```py
from sklearn.preprocessing import MinMaxScaler
m_scaler = MinMaxScaler()

# 최소 / 최대값 통계량을 설정
m_scaler.fit(iris_df) 

# transform() 메서드를 사용하여 데이터를 스케일링합니다.
iris_sc_mx =  m_scaler.transform(iris_df)

# iris_df에 Min-Max 스케일링을 적용 ->  iris_sc_mx에 스케일링된 결과를 저장 -> 이를 다시 iris_mx_df 데이터 프레임으로 변환
iris_mx_df = pd.DataFrame(iris_sc_mx, columns = iris.feature_names)

# 기존값과 변화값의 차이를 비교해준다.
print('특성의 최대값 : ', iris_mx_df.max())
print('특성의 최소값 : ', iris_mx_df.min())
```

### 모델링에 가장 적합한 확률 분포 : 정규분포

- 수집된 data는 특정방향으로 치우쳐져 있음

- 한쪽으로 치우친 변수에 대해서는 `반대방향의 값(꼬리부분)`들이 이상치처럼 작용할 수 있으므로 이러한 치우침을 제거해야  -> `왜도를 0에 가깝게` 만들어준다. 

#### 탐색방법 : 왜도(skewness)

- 변수 치우침 확인하는 가장 적절한 척도, 한쪽으로 치우쳐 있는 정도를 표현함.

- 왜도 : 분포의 비대칭도를 나타내는 통계량
    - df.skew(), np.skew(array) 
    - 보통 왜도의 절대값이 1.5이상이면 치우쳐졌다고 판단
    - 이상치 데이터로 작용할 수 있으므로 왜도를 조정해줘야한다.
    - log 변환을 통해 비대칭도를 제거한다.

```py
import pandas as pd

# header 없다고 설정하면 0 부터 순서를 매기는 column 이 자동으로 생성
df = pd.read_csv('./데이터/sonar.csv', header = None)

# column 명이 없는 data 
# 데이터프레임의 컬럼 이름을 설정
# 컬럼 이름은 'band1'에서 'band60'까지의 60개의 컬럼을 추가
columns = ['band' + str(i) for i in range(1, 61)]

# 'y' 컬럼을 추가
columns.append('y')

# 'y' 컬럼을 타겟 데이터로 설정, 나머지 컬럼을 특성 데이터 X로 설정
X = df.drop('y', axis = 1)
y = df['y']

# X.skew()는 특성 데이터 X의 각 컬럼에 대한 왜도(Skewness)를 계산
X.skew()
```

```py
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# 'band4' 열의 히스토그램을 그린다.
df['band4'].hist()

# 'band4' 열과 같이 왜도가 높은 열(치우침이 심한 열)을 선택한다. 
# 왜도의 절대값이 1.5보다 큰 열을 선택하고 왜도가 1.5를 초과하면 해당 열은 치우친 데이터로 간주됩니다.
bias_var = X.columns[X.skew().abs() > 1.5]

# 선택된 (치우친) 열에 대해 로그 변환을 수행
# 로그 변환을 통해 치우침을 줄이고 정규분포에 가깝게 만들기 위해서
# 로그변환한 자료를 히스토그램을 그려 왜도를 확인
np.log2(X['band4']).hist()

# 치우침이 제거된 열을 다시 데이터프레임에 할당합니다.
X[bias_var] = np.log2(X[bias_var])

# 치우친 변수명을 확인: 치추친 변수에 대해서는 log 변환 진행된 data 이므로 왜도 절대값이 1.5 초과 컬럼은 없다.
X.columns[X.skew().abs() > 1.5]
```
#### 주의사항

- 모든 데이터가 log 변환 후 왜도값이 1.5 이하로 떨어지는 건 아님
  
  - 왜도는 데이터의 분포 형태에 대한 정보를 제공한다.
  
  - 로그 변환은 치우침을 완전히 제거하지 못할 수 있다.
  
  - 몇몇 변수는 치우침을 가지고 있는 것이 자연스러운 경우일 수 있습니다.

- 치우침을 제거했을 때 알고리즘 성능이 절대적으로 좋아지는 건 아니다. 
  
  - 치우침을 제거하는 것은 데이터 전처리 단계 중 하나일 뿐, 성능 향상이 보장되지는 않는다.
  
  - 다만, 치우침이 있는 데이터의 경우 알고리즘의 학습이나 예측에 부정적인 영향을 미칠 수 있으므로 성능 향상의 여지를 제공할 수 있습니다.

- 데이터(특정) 중 돈과 관련된 특성은 log 변환
  
  - 돈과 관련된 특성은 종종 큰 값의 차이가 발생하고, 로그 변환은 이러한 값의 차이를 줄일 수 있습니다. 
  
  - 로그 변환을 통해 데이터의 스케일을 조정하고 모델의 성능을 향상시킬 수 있습니다.