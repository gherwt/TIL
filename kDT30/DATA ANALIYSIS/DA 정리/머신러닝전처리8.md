### 피처 스케일링과 정규화 
- 피처 스케일링
    - 서로 다른 변수의 값 범위를 일정한 범위 수준으로 맞추는 작업
- 방식
    - Z-scaling
        - 표준화
        - 평균이 0이고 분산이 1인 가우시안 정규분포로 변화나는 과정
        - sklearn.preprocessing의 StandardScaler 모듈
        - 사전에 표준화 작업을 반드시 진행하는 알고리즘
            - 회귀(선형회귀(연속값 예측 회귀), 로지스틱회귀(분류))알고리즘
            - SVM 
    
    - Min-Max scaling
        - 0 ~ 1 사이의 값으로 변환
        - 최소값을 0으로 변환, 최대값을 1로 변환
        - sklearn.preprocessing 의 MinMaxScaler 모듈을 사용
        - x_new = (xi - min(x)) / (max(x) - min(x))
### StandardScaler 사용 예제
from sklearn.datasets import load_iris # iris 데이터 저장 모듈
import pandas as pd
iris = load_iris()
iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names)
iris_df.head(3)
from sklearn.preprocessing import StandardScaler
# 객체를 생성
scaler = StandardScaler()
scaler.fit(iris_df) # 각 column 의 평균과 표준편차를 계산(통계량을 계산)
iris_sc_df = pd.DataFrame(data = scaler.transform(iris_df), columns = iris.feature_names)
iris_sc_df
print('변환 전 특성의 평균값 :', iris_df.mean())
print('변환 후 특성의 평균값 :', iris_sc_df.mean())
### MinMaxScaler
- 데이터의 정규 분포가 가우시안 분포와 너무 관련이 없을 때
- 표준화를 진행해도 관련이 없을 때 Min-Max-Scale 적용
- 도메인적으로 해당 칼럼(특성)의 최소값과 최대값은 변경되지 않는다는 것이 어느정도 신뢰되면 사용한다.
from sklearn.preprocessing import MinMaxScaler
m_scaler = MinMaxScaler()
m_scaler.fit(iris_df) # 최소 / 최대값 통계량을 설정
iris_sc_mx =  m_scaler.transform(iris_df)
iris_mx_df = pd.DataFrame(iris_sc_mx, columns = iris.feature_names)
iris_mx_df.head(5)
print('특성의 최대값 : ', iris_mx_df.max())
print('특성의 최소값 : ', iris_mx_df.min())


- 모델링에 가장 적합한 확률 분포 : 정규분포
    - 수집된 data는 특정방향으로 치우쳐져 있음
- 한쪽으로 치우친 변수에 대해서는 반대방향의 값(꼬리부분)들이 이상치처럼 작용할 수 있으므로 이러한 치우침을 제거해야  -> 왜도를 0에 가깝게 만들어준다. 
### 탐색방법 : 왜도(skewness)
- 변수 치우침 확인하는 가장 적절한 척도
- 왜도 : 분포의 비대칭도를 나타내는 통계량
    - df.skew(), np.skew(array) 
    - 보통 왜도의 절대값이 1.5이상이면 치우쳐졌다고 판단
    - 이상치 데이터로 작용할 수 있으므로 왜도를 조정해줘야한다.
    - log 변환을 통해 비대칭도를 제거한다.
![Image20231024132801.png](attachment:5d455402-c562-42d1-bf18-01476495b139.png)
import pandas as pd
df = pd.read_csv('./데이터/sonar.csv', header = None)
# header 없다고 설정하면 0 부터 순서를 매기는 column 이 자동으로 생성
# column 명이 없는 data 
df.head()
df.shape
columns = ['band' + str(i) for i in range(1, 61)]
columns.append('y')
df.columns = columns
df.head()
X = df.drop('y', axis = 1)
y = df['y']
X.skew()
%matplotlib inline
import matplotlib.pyplot as plt
df['band4'].hist()
# 왜도 기반 치우친 변수에 대해 치우침을 제거 후 사용
# log 변환을 진행해서 치우침을 제거
import numpy as np
bias_var = X.columns[X.skew().abs() > 1.5] # log 변환 대상 컬럼(왜도가 심한 컬럼명)
# 왜도가 가장 큰 band4에 대해 log 변환 후 hist 확인
np.log2(X['band4']).hist()
X[bias_var] = np.log2(X[bias_var])
X[bias_var].head()
# 치우친 변수명을 확인: 치추친 변수에 대해서는 log 변환 진행된 data 이므로 왜도 절대값이 1.5 초과 컬럼은 없다.
X.columns[X.skew().abs() > 1.5]
- tip.
    - 모든 데이터가 log 변환 후 왜도값이 1.5 이하로 떨어지는 건 아님
    - 치우침을 제거했을 때 알고리즘 성능이 절대적으로 좋아지는 건 아니다. 다만, 성능이 좋아질 여지는 있다.
    - 데이터(특정) 중 돈과 관련된 특성은 log 변환이 데이터가 모델에 미치는 영향을 높인다고 알려져 있다. -> 다가오는 값어치의 차이가 발생하기 때문에